{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Loss Functions in Machine Learning\n",
        "\n",
        "This notebook explores three fundamental loss functions used in machine learning:\n",
        "1. **Binary Cross-Entropy Loss** - for binary classification\n",
        "2. **Cross-Entropy Loss** - for multi-class classification\n",
        "3. **Sum of Squared Residuals (SSR)** - for regression tasks\n",
        "\n",
        "We'll cover the mathematical formulations, implementations, and practical examples for each."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import make_classification, make_regression\n",
        "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style for better plots\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Binary Cross-Entropy Loss\n",
        "\n",
        "Binary Cross-Entropy Loss is used for binary classification problems where we need to predict one of two classes (0 or 1).\n",
        "\n",
        "### Mathematical Formula\n",
        "\n",
        "For a single sample:\n",
        "$$BCE = -[y \\log(\\hat{y}) + (1-y) \\log(1-\\hat{y})]$$\n",
        "\n",
        "For multiple samples:\n",
        "$$BCE = -\\frac{1}{n} \\sum_{i=1}^{n} [y_i \\log(\\hat{y_i}) + (1-y_i) \\log(1-\\hat{y_i})]$$\n",
        "\n",
        "Where:\n",
        "- $y$ is the true binary label (0 or 1)\n",
        "- $\\hat{y}$ is the predicted probability (between 0 and 1)\n",
        "- $n$ is the number of samples\n",
        "\n",
        "### Key Properties\n",
        "- **Range**: [0, ∞)\n",
        "- **Minimum**: 0 (perfect prediction)\n",
        "- **Behavior**: Heavily penalizes confident wrong predictions\n",
        "- **Use case**: Binary classification with sigmoid activation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def binary_cross_entropy(y_true, y_pred, epsilon=1e-15):\n",
        "    \"\"\"\n",
        "    Calculate binary cross-entropy loss\n",
        "    \n",
        "    Args:\n",
        "        y_true: True binary labels (0 or 1)\n",
        "        y_pred: Predicted probabilities (0 to 1)\n",
        "        epsilon: Small value to prevent log(0)\n",
        "    \n",
        "    Returns:\n",
        "        Binary cross-entropy loss\n",
        "    \"\"\"\n",
        "    # Clip predictions to prevent log(0)\n",
        "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
        "    \n",
        "    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
        "\n",
        "# Example calculation\n",
        "y_true = np.array([1, 0, 1, 1, 0])\n",
        "y_pred = np.array([0.9, 0.1, 0.8, 0.7, 0.3])\n",
        "\n",
        "bce_loss = binary_cross_entropy(y_true, y_pred)\n",
        "print(f\"Binary Cross-Entropy Loss: {bce_loss:.4f}\")\n",
        "\n",
        "# Show individual contributions\n",
        "individual_losses = -(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
        "print(\"\\nIndividual sample losses:\")\n",
        "for i, (true, pred, loss) in enumerate(zip(y_true, y_pred, individual_losses)):\n",
        "    print(f\"Sample {i+1}: y_true={true}, y_pred={pred:.1f}, loss={loss:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize Binary Cross-Entropy Loss\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# Loss curve for different predictions when y_true = 1\n",
        "y_pred_range = np.linspace(0.001, 0.999, 1000)\n",
        "loss_y1 = -np.log(y_pred_range)  # When y_true = 1\n",
        "loss_y0 = -np.log(1 - y_pred_range)  # When y_true = 0\n",
        "\n",
        "ax1.plot(y_pred_range, loss_y1, label='y_true = 1', linewidth=2)\n",
        "ax1.plot(y_pred_range, loss_y0, label='y_true = 0', linewidth=2)\n",
        "ax1.set_xlabel('Predicted Probability')\n",
        "ax1.set_ylabel('Binary Cross-Entropy Loss')\n",
        "ax1.set_title('BCE Loss vs Predicted Probability')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "ax1.set_ylim(0, 5)\n",
        "\n",
        "# Practical example with real data\n",
        "X, y = make_classification(n_samples=1000, n_features=2, n_redundant=0, \n",
        "                          n_informative=2, n_clusters_per_class=1, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train logistic regression\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate loss\n",
        "test_loss = binary_cross_entropy(y_test, y_pred_proba)\n",
        "\n",
        "# Plot decision boundary\n",
        "h = 0.02\n",
        "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
        "Z = model.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
        "Z = Z.reshape(xx.shape)\n",
        "\n",
        "ax2.contourf(xx, yy, Z, levels=50, alpha=0.6, cmap='RdYlBu')\n",
        "scatter = ax2.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap='RdYlBu', edgecolors='black')\n",
        "ax2.set_xlabel('Feature 1')\n",
        "ax2.set_ylabel('Feature 2')\n",
        "ax2.set_title(f'Binary Classification\\nTest BCE Loss: {test_loss:.4f}')\n",
        "plt.colorbar(scatter, ax=ax2)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Cross-Entropy Loss (Multi-class)\n",
        "\n",
        "Cross-Entropy Loss is used for multi-class classification problems where we need to predict one of multiple classes.\n",
        "\n",
        "### Mathematical Formula\n",
        "\n",
        "For a single sample:\n",
        "$$CE = -\\sum_{c=1}^{C} y_c \\log(\\hat{y_c})$$\n",
        "\n",
        "For multiple samples:\n",
        "$$CE = -\\frac{1}{n} \\sum_{i=1}^{n} \\sum_{c=1}^{C} y_{i,c} \\log(\\hat{y_{i,c}})$$\n",
        "\n",
        "Where:\n",
        "- $C$ is the number of classes\n",
        "- $y_{i,c}$ is 1 if sample $i$ belongs to class $c$, 0 otherwise (one-hot encoding)\n",
        "- $\\hat{y_{i,c}}$ is the predicted probability that sample $i$ belongs to class $c$\n",
        "- $n$ is the number of samples\n",
        "\n",
        "### Key Properties\n",
        "- **Range**: [0, ∞)\n",
        "- **Minimum**: 0 (perfect prediction)\n",
        "- **Use case**: Multi-class classification with softmax activation\n",
        "- **Reduces to**: Binary cross-entropy when C=2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def softmax(x):\n",
        "    \"\"\"\n",
        "    Softmax activation function\n",
        "    \"\"\"\n",
        "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))  # Numerical stability\n",
        "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
        "\n",
        "def cross_entropy_loss(y_true, y_pred, epsilon=1e-15):\n",
        "    \"\"\"\n",
        "    Calculate cross-entropy loss for multi-class classification\n",
        "    \n",
        "    Args:\n",
        "        y_true: True labels (one-hot encoded or class indices)\n",
        "        y_pred: Predicted probabilities (after softmax)\n",
        "        epsilon: Small value to prevent log(0)\n",
        "    \n",
        "    Returns:\n",
        "        Cross-entropy loss\n",
        "    \"\"\"\n",
        "    # Convert class indices to one-hot if needed\n",
        "    if y_true.ndim == 1:\n",
        "        y_true_onehot = np.eye(y_pred.shape[1])[y_true]\n",
        "    else:\n",
        "        y_true_onehot = y_true\n",
        "    \n",
        "    # Clip predictions to prevent log(0)\n",
        "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
        "    \n",
        "    return -np.mean(np.sum(y_true_onehot * np.log(y_pred), axis=1))\n",
        "\n",
        "# Example with 3 classes\n",
        "n_samples, n_classes = 5, 3\n",
        "y_true = np.array([0, 1, 2, 0, 1])  # Class indices\n",
        "\n",
        "# Simulate logits (pre-softmax outputs)\n",
        "logits = np.array([\n",
        "    [2.0, 1.0, 0.1],   # Should predict class 0\n",
        "    [0.5, 2.5, 0.2],   # Should predict class 1  \n",
        "    [0.1, 0.3, 2.8],   # Should predict class 2\n",
        "    [1.8, 1.2, 0.5],   # Should predict class 0\n",
        "    [0.3, 2.1, 0.8]    # Should predict class 1\n",
        "])\n",
        "\n",
        "y_pred = softmax(logits)\n",
        "ce_loss = cross_entropy_loss(y_true, y_pred)\n",
        "\n",
        "print(f\"Cross-Entropy Loss: {ce_loss:.4f}\")\n",
        "print(\"\\nPredicted probabilities:\")\n",
        "for i, (true_class, pred_probs) in enumerate(zip(y_true, y_pred)):\n",
        "    pred_class = np.argmax(pred_probs)\n",
        "    confidence = pred_probs[pred_class]\n",
        "    print(f\"Sample {i+1}: True={true_class}, Pred={pred_class}, Confidence={confidence:.3f}, Probs={pred_probs}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize Cross-Entropy Loss with real data\n",
        "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "# Generate multi-class dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=2, n_redundant=0, \n",
        "                          n_informative=2, n_clusters_per_class=1, \n",
        "                          n_classes=3, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train logistic regression for multi-class\n",
        "model_mc = LogisticRegression(multi_class='ovr', max_iter=1000)\n",
        "model_mc.fit(X_train, y_train)\n",
        "y_pred_proba_mc = model_mc.predict_proba(X_test)\n",
        "\n",
        "# Calculate loss\n",
        "test_loss_mc = cross_entropy_loss(y_test, y_pred_proba_mc)\n",
        "\n",
        "# Plot 1: Loss evolution during training (simulated)\n",
        "epochs = np.arange(1, 51)\n",
        "train_losses = 2.5 * np.exp(-epochs/15) + 0.3 + 0.1 * np.random.normal(0, 0.1, len(epochs))\n",
        "val_losses = 2.8 * np.exp(-epochs/18) + 0.4 + 0.15 * np.random.normal(0, 0.1, len(epochs))\n",
        "\n",
        "ax1.plot(epochs, train_losses, label='Training Loss', linewidth=2)\n",
        "ax1.plot(epochs, val_losses, label='Validation Loss', linewidth=2)\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Cross-Entropy Loss')\n",
        "ax1.set_title('Training History')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: Decision boundaries\n",
        "h = 0.02\n",
        "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
        "Z = model_mc.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z = Z.reshape(xx.shape)\n",
        "\n",
        "ax2.contourf(xx, yy, Z, alpha=0.6, cmap='viridis')\n",
        "scatter = ax2.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap='viridis', edgecolors='black')\n",
        "ax2.set_xlabel('Feature 1')\n",
        "ax2.set_ylabel('Feature 2')\n",
        "ax2.set_title(f'Multi-class Classification\\nTest CE Loss: {test_loss_mc:.4f}')\n",
        "plt.colorbar(scatter, ax=ax2)\n",
        "\n",
        "# Plot 3: Confidence distribution\n",
        "max_probs = np.max(y_pred_proba_mc, axis=1)\n",
        "correct_preds = (model_mc.predict(X_test) == y_test)\n",
        "\n",
        "ax3.hist(max_probs[correct_preds], bins=20, alpha=0.7, label='Correct', density=True)\n",
        "ax3.hist(max_probs[~correct_preds], bins=20, alpha=0.7, label='Incorrect', density=True)\n",
        "ax3.set_xlabel('Maximum Predicted Probability')\n",
        "ax3.set_ylabel('Density')\n",
        "ax3.set_title('Prediction Confidence Distribution')\n",
        "ax3.legend()\n",
        "ax3.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 4: Loss vs confidence\n",
        "individual_losses = []\n",
        "for i in range(len(y_test)):\n",
        "    true_class = y_test[i]\n",
        "    pred_prob = y_pred_proba_mc[i, true_class]\n",
        "    individual_losses.append(-np.log(pred_prob))\n",
        "\n",
        "ax4.scatter(max_probs, individual_losses, alpha=0.6, c=correct_preds, cmap='RdYlGn')\n",
        "ax4.set_xlabel('Maximum Predicted Probability')\n",
        "ax4.set_ylabel('Individual CE Loss')\n",
        "ax4.set_title('Loss vs Confidence')\n",
        "ax4.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Comparison and Applications\n",
        "\n",
        "### When to Use Each Loss Function\n",
        "\n",
        "| Loss Function | Problem Type | Output Layer | Key Characteristics |\n",
        "|---------------|--------------|--------------|--------------------|\n",
        "| **Binary Cross-Entropy** | Binary Classification | Sigmoid | - Outputs probabilities [0,1]<br>- Heavily penalizes confident wrong predictions<br>- Suitable for imbalanced datasets |\n",
        "| **Cross-Entropy** | Multi-class Classification | Softmax | - Outputs probability distribution<br>- Only penalizes incorrect class prediction<br>- Works well with one-hot encoded targets |\n",
        "| **Sum of Squared Residuals** | Regression | Linear/None | - Sensitive to outliers<br>- Assumes Gaussian noise<br>- Differentiable everywhere |\n",
        "\n",
        "### Mathematical Relationships\n",
        "\n",
        "1. **Binary Cross-Entropy** is a special case of **Cross-Entropy** when there are only 2 classes\n",
        "2. **MSE** is related to **Maximum Likelihood Estimation** under Gaussian assumptions\n",
        "3. All three functions are **convex**, ensuring global optima for linear models\n",
        "\n",
        "### Practical Considerations\n",
        "\n",
        "- **Numerical Stability**: Always clip probabilities to avoid log(0)\n",
        "- **Class Imbalance**: Consider weighted versions or alternative metrics\n",
        "- **Outliers**: MSE is sensitive to outliers; consider Huber loss or MAE as alternatives\n",
        "- **Interpretability**: Cross-entropy measures \"surprise\", MSE measures average squared deviation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Practical example: Comparing loss behaviors\n",
        "def compare_loss_behaviors():\n",
        "    \"\"\"\n",
        "    Demonstrate how different loss functions behave with various prediction scenarios\n",
        "    \"\"\"\n",
        "    print(\"=\" * 80)\n",
        "    print(\"LOSS FUNCTION COMPARISON\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    # Scenario 1: Good predictions\n",
        "    print(\"\\n1. GOOD PREDICTIONS:\")\n",
        "    y_true_bin = np.array([1, 0, 1, 0])\n",
        "    y_pred_bin = np.array([0.9, 0.1, 0.85, 0.15])\n",
        "    \n",
        "    y_true_mc = np.array([0, 1, 2, 0])\n",
        "    y_pred_mc = np.array([[0.8, 0.1, 0.1], [0.1, 0.8, 0.1], \n",
        "                         [0.1, 0.1, 0.8], [0.7, 0.2, 0.1]])\n",
        "    \n",
        "    y_true_reg = np.array([2.0, -1.0, 3.5, 0.8])\n",
        "    y_pred_reg = np.array([2.1, -0.9, 3.4, 0.9])\n",
        "    \n",
        "    print(f\"Binary Cross-Entropy: {binary_cross_entropy(y_true_bin, y_pred_bin):.4f}\")\n",
        "    print(f\"Multi-class Cross-Entropy: {cross_entropy_loss(y_true_mc, y_pred_mc):.4f}\")\n",
        "    print(f\"MSE: {mean_squared_error(y_true_reg, y_pred_reg):.4f}\")\n",
        "    \n",
        "    # Scenario 2: Poor predictions\n",
        "    print(\"\\n2. POOR PREDICTIONS:\")\n",
        "    y_pred_bin_poor = np.array([0.2, 0.8, 0.3, 0.7])\n",
        "    y_pred_mc_poor = np.array([[0.2, 0.4, 0.4], [0.5, 0.3, 0.2], \n",
        "                              [0.4, 0.4, 0.2], [0.3, 0.4, 0.3]])\n",
        "    y_pred_reg_poor = np.array([0.5, 1.2, 1.8, 2.5])\n",
        "    \n",
        "    print(f\"Binary Cross-Entropy: {binary_cross_entropy(y_true_bin, y_pred_bin_poor):.4f}\")\n",
        "    print(f\"Multi-class Cross-Entropy: {cross_entropy_loss(y_true_mc, y_pred_mc_poor):.4f}\")\n",
        "    print(f\"MSE: {mean_squared_error(y_true_reg, y_pred_reg_poor):.4f}\")\n",
        "    \n",
        "    # Scenario 3: Overconfident wrong predictions\n",
        "    print(\"\\n3. OVERCONFIDENT WRONG PREDICTIONS:\")\n",
        "    y_pred_bin_conf = np.array([0.05, 0.95, 0.1, 0.9])  # Very confident but wrong\n",
        "    y_pred_mc_conf = np.array([[0.05, 0.05, 0.9], [0.9, 0.05, 0.05], \n",
        "                              [0.9, 0.05, 0.05], [0.05, 0.9, 0.05]])\n",
        "    y_pred_reg_conf = np.array([5.0, -4.0, 8.0, -2.0])  # Large errors\n",
        "    \n",
        "    print(f\"Binary Cross-Entropy: {binary_cross_entropy(y_true_bin, y_pred_bin_conf):.4f}\")\n",
        "    print(f\"Multi-class Cross-Entropy: {cross_entropy_loss(y_true_mc, y_pred_mc_conf):.4f}\")\n",
        "    print(f\"MSE: {mean_squared_error(y_true_reg, y_pred_reg_conf):.4f}\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"OBSERVATIONS:\")\n",
        "    print(\"- Cross-entropy losses explode with overconfident wrong predictions\")\n",
        "    print(\"- MSE grows quadratically with error magnitude\")\n",
        "    print(\"- All losses are minimized when predictions match true values\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "compare_loss_behaviors()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Advanced Topics\n",
        "\n",
        "### 1. Weighted Loss Functions\n",
        "For imbalanced datasets, you can weight the loss function to give more importance to minority classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def weighted_binary_cross_entropy(y_true, y_pred, class_weights=None, epsilon=1e-15):\n",
        "    \"\"\"\n",
        "    Weighted Binary Cross-Entropy Loss\n",
        "    \n",
        "    Args:\n",
        "        y_true: True binary labels\n",
        "        y_pred: Predicted probabilities\n",
        "        class_weights: Dict with weights for each class {0: weight_0, 1: weight_1}\n",
        "    \"\"\"\n",
        "    if class_weights is None:\n",
        "        class_weights = {0: 1.0, 1: 1.0}\n",
        "    \n",
        "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
        "    \n",
        "    # Apply weights\n",
        "    weights = np.where(y_true == 1, class_weights[1], class_weights[0])\n",
        "    \n",
        "    loss = -(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
        "    return np.mean(weights * loss)\n",
        "\n",
        "# Example with imbalanced data\n",
        "y_true_imb = np.array([0, 0, 0, 0, 0, 0, 0, 0, 1, 1])  # 80% class 0, 20% class 1\n",
        "y_pred_imb = np.array([0.1, 0.2, 0.15, 0.3, 0.25, 0.1, 0.2, 0.3, 0.8, 0.7])\n",
        "\n",
        "# Standard loss\n",
        "standard_loss = binary_cross_entropy(y_true_imb, y_pred_imb)\n",
        "\n",
        "# Weighted loss (give more weight to minority class)\n",
        "weighted_loss = weighted_binary_cross_entropy(y_true_imb, y_pred_imb, \n",
        "                                            class_weights={0: 1.0, 1: 4.0})\n",
        "\n",
        "print(f\"Standard BCE Loss: {standard_loss:.4f}\")\n",
        "print(f\"Weighted BCE Loss: {weighted_loss:.4f}\")\n",
        "print(f\"Class distribution: {np.bincount(y_true_imb.astype(int))}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Focal Loss\n",
        "Focal Loss addresses class imbalance by down-weighting easy examples and focusing on hard examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def focal_loss(y_true, y_pred, alpha=1.0, gamma=2.0, epsilon=1e-15):\n",
        "    \"\"\"\n",
        "    Focal Loss for binary classification\n",
        "    \n",
        "    Args:\n",
        "        y_true: True binary labels\n",
        "        y_pred: Predicted probabilities\n",
        "        alpha: Weighting factor for class imbalance\n",
        "        gamma: Focusing parameter (higher gamma focuses more on hard examples)\n",
        "    \"\"\"\n",
        "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
        "    \n",
        "    # Calculate p_t\n",
        "    p_t = np.where(y_true == 1, y_pred, 1 - y_pred)\n",
        "    \n",
        "    # Calculate alpha_t\n",
        "    alpha_t = np.where(y_true == 1, alpha, 1 - alpha)\n",
        "    \n",
        "    # Focal loss formula\n",
        "    focal_loss = -alpha_t * (1 - p_t) ** gamma * np.log(p_t)\n",
        "    \n",
        "    return np.mean(focal_loss)\n",
        "\n",
        "# Compare different loss functions on the same data\n",
        "focal_loss_val = focal_loss(y_true_imb, y_pred_imb, alpha=0.25, gamma=2.0)\n",
        "\n",
        "print(f\"Standard BCE Loss: {standard_loss:.4f}\")\n",
        "print(f\"Weighted BCE Loss: {weighted_loss:.4f}\")\n",
        "print(f\"Focal Loss: {focal_loss_val:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Custom Loss Function Implementation\n",
        "Here's how you might implement these loss functions in a deep learning framework like TensorFlow/Keras:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pseudo-code for TensorFlow/Keras implementation\n",
        "tensorflow_code = '''\n",
        "import tensorflow as tf\n",
        "\n",
        "def custom_binary_crossentropy(y_true, y_pred):\n",
        "    \"\"\"Custom binary cross-entropy loss for Keras\"\"\"\n",
        "    epsilon = tf.keras.backend.epsilon()\n",
        "    y_pred = tf.clip_by_value(y_pred, epsilon, 1 - epsilon)\n",
        "    return -tf.reduce_mean(y_true * tf.math.log(y_pred) + \n",
        "                          (1 - y_true) * tf.math.log(1 - y_pred))\n",
        "\n",
        "def custom_categorical_crossentropy(y_true, y_pred):\n",
        "    \"\"\"Custom categorical cross-entropy loss for Keras\"\"\"\n",
        "    epsilon = tf.keras.backend.epsilon()\n",
        "    y_pred = tf.clip_by_value(y_pred, epsilon, 1 - epsilon)\n",
        "    return -tf.reduce_mean(tf.reduce_sum(y_true * tf.math.log(y_pred), axis=1))\n",
        "\n",
        "def custom_mse(y_true, y_pred):\n",
        "    \"\"\"Custom mean squared error loss for Keras\"\"\"\n",
        "    return tf.reduce_mean(tf.square(y_true - y_pred))\n",
        "\n",
        "# Usage in model compilation\n",
        "model.compile(optimizer='adam', \n",
        "              loss=custom_binary_crossentropy,  # or custom_categorical_crossentropy, custom_mse\n",
        "              metrics=['accuracy'])\n",
        "'''\n",
        "\n",
        "print(\"TensorFlow/Keras Implementation Example:\")\n",
        "print(tensorflow_code)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook covered three fundamental loss functions in machine learning:\n",
        "\n",
        "### Key Takeaways:\n",
        "\n",
        "1. **Binary Cross-Entropy Loss**:\n",
        "   - Used for binary classification problems\n",
        "   - Measures the difference between predicted probabilities and true binary labels\n",
        "   - Heavily penalizes confident wrong predictions\n",
        "\n",
        "2. **Cross-Entropy Loss (Multi-class)**:\n",
        "   - Used for multi-class classification problems\n",
        "   - Works with softmax activation to produce probability distributions\n",
        "   - Only penalizes the probability assigned to the correct class\n",
        "\n",
        "3. **Sum of Squared Residuals (SSR)**:\n",
        "   - Used for regression problems\n",
        "   - Measures the sum of squared differences between predictions and true values\n",
        "   - Sensitive to outliers due to quadratic penalty\n",
        "\n",
        "### Best Practices:\n",
        "\n",
        "- Always clip probabilities to prevent numerical issues with log(0)\n",
        "- Consider class weights for imbalanced datasets\n",
        "- Use appropriate activation functions (sigmoid for binary, softmax for multi-class)\n",
        "- Monitor validation loss to detect overfitting\n",
        "- Consider alternative loss functions (Focal Loss, Huber Loss) for specific challenges\n",
        "\n",
        "### Further Reading:\n",
        "\n",
        "- Information Theory and Cross-Entropy\n",
        "- Maximum Likelihood Estimation\n",
        "- Regularization Techniques\n",
        "- Advanced Loss Functions (Focal Loss, Dice Loss, etc.)\n",
        "- Loss Function Selection for Different Problem Types"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Sum of Squared Residuals (SSR)\n",
        "\n",
        "Sum of Squared Residuals, also known as Sum of Squared Errors (SSE), is used for regression problems where we predict continuous values.\n",
        "\n",
        "### Mathematical Formula\n",
        "\n",
        "$$SSR = \\sum_{i=1}^{n} (y_i - \\hat{y_i})^2$$\n",
        "\n",
        "Mean Squared Error (MSE), which is the average:\n",
        "$$MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y_i})^2$$\n",
        "\n",
        "Root Mean Squared Error (RMSE):\n",
        "$$RMSE = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y_i})^2}$$\n",
        "\n",
        "Where:\n",
        "- $y_i$ is the true value for sample $i$\n",
        "- $\\hat{y_i}$ is the predicted value for sample $i$\n",
        "- $n$ is the number of samples\n",
        "\n",
        "### Key Properties\n",
        "- **Range**: [0, ∞)\n",
        "- **Minimum**: 0 (perfect prediction)\n",
        "- **Units**: MSE has squared units, RMSE has same units as target\n",
        "- **Sensitivity**: Heavily penalizes large errors (quadratic penalty)\n",
        "- **Use case**: Regression tasks, assumes Gaussian noise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def sum_squared_residuals(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Calculate Sum of Squared Residuals\n",
        "    \n",
        "    Args:\n",
        "        y_true: True values\n",
        "        y_pred: Predicted values\n",
        "    \n",
        "    Returns:\n",
        "        SSR value\n",
        "    \"\"\"\n",
        "    return np.sum((y_true - y_pred) ** 2)\n",
        "\n",
        "def mean_squared_error(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Calculate Mean Squared Error\n",
        "    \"\"\"\n",
        "    return np.mean((y_true - y_pred) ** 2)\n",
        "\n",
        "def root_mean_squared_error(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Calculate Root Mean Squared Error\n",
        "    \"\"\"\n",
        "    return np.sqrt(np.mean((y_true - y_pred) ** 2))\n",
        "\n",
        "# Example calculation\n",
        "y_true = np.array([3.0, -0.5, 2.0, 7.0, 4.2])\n",
        "y_pred = np.array([2.5, 0.0, 2.1, 7.8, 4.0])\n",
        "\n",
        "ssr = sum_squared_residuals(y_true, y_pred)\n",
        "mse = mean_squared_error(y_true, y_pred)\n",
        "rmse = root_mean_squared_error(y_true, y_pred)\n",
        "\n",
        "print(f\"Sum of Squared Residuals: {ssr:.4f}\")\n",
        "print(f\"Mean Squared Error: {mse:.4f}\")\n",
        "print(f\"Root Mean Squared Error: {rmse:.4f}\")\n",
        "\n",
        "# Show individual contributions\n",
        "residuals = y_true - y_pred\n",
        "squared_residuals = residuals ** 2\n",
        "\n",
        "print(\"\\nIndividual sample analysis:\")\n",
        "for i, (true, pred, res, sq_res) in enumerate(zip(y_true, y_pred, residuals, squared_residuals)):\n",
        "    print(f\"Sample {i+1}: y_true={true:4.1f}, y_pred={pred:4.1f}, residual={res:5.2f}, squared={sq_res:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize Sum of Squared Residuals with real data\n",
        "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "# Generate regression dataset\n",
        "X_reg, y_reg = make_regression(n_samples=100, n_features=1, noise=10, random_state=42)\n",
        "X_reg = X_reg.flatten()\n",
        "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n",
        "    X_reg.reshape(-1, 1), y_reg, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Train linear regression\n",
        "model_reg = LinearRegression()\n",
        "model_reg.fit(X_train_reg, y_train_reg)\n",
        "y_pred_reg = model_reg.predict(X_test_reg)\n",
        "\n",
        "# Calculate metrics\n",
        "ssr_test = sum_squared_residuals(y_test_reg, y_pred_reg)\n",
        "mse_test = mean_squared_error(y_test_reg, y_pred_reg)\n",
        "rmse_test = root_mean_squared_error(y_test_reg, y_pred_reg)\n",
        "\n",
        "# Plot 1: Regression line with residuals\n",
        "X_plot = np.linspace(X_reg.min(), X_reg.max(), 100).reshape(-1, 1)\n",
        "y_plot = model_reg.predict(X_plot)\n",
        "\n",
        "ax1.scatter(X_test_reg.flatten(), y_test_reg, alpha=0.6, label='Test Data')\n",
        "ax1.plot(X_plot.flatten(), y_plot, 'r-', linewidth=2, label='Regression Line')\n",
        "\n",
        "# Draw residual lines\n",
        "for i in range(len(X_test_reg)):\n",
        "    ax1.plot([X_test_reg[i], X_test_reg[i]], [y_test_reg[i], y_pred_reg[i]], \n",
        "             'r--', alpha=0.5, linewidth=1)\n",
        "\n",
        "ax1.set_xlabel('X')\n",
        "ax1.set_ylabel('y')\n",
        "ax1.set_title(f'Linear Regression with Residuals\\nSSR: {ssr_test:.2f}, MSE: {mse_test:.2f}, RMSE: {rmse_test:.2f}')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: Residual plot\n",
        "residuals_test = y_test_reg - y_pred_reg\n",
        "ax2.scatter(y_pred_reg, residuals_test, alpha=0.6)\n",
        "ax2.axhline(y=0, color='r', linestyle='--', linewidth=2)\n",
        "ax2.set_xlabel('Predicted Values')\n",
        "ax2.set_ylabel('Residuals')\n",
        "ax2.set_title('Residual Plot')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 3: Distribution of residuals\n",
        "ax3.hist(residuals_test, bins=15, alpha=0.7, density=True, color='skyblue')\n",
        "ax3.axvline(x=0, color='r', linestyle='--', linewidth=2)\n",
        "ax3.set_xlabel('Residuals')\n",
        "ax3.set_ylabel('Density')\n",
        "ax3.set_title('Distribution of Residuals')\n",
        "ax3.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 4: Different loss functions comparison\n",
        "error_range = np.linspace(-3, 3, 100)\n",
        "squared_loss = error_range ** 2\n",
        "absolute_loss = np.abs(error_range)\n",
        "huber_loss = np.where(np.abs(error_range) <= 1, \n",
        "                     0.5 * error_range ** 2, \n",
        "                     np.abs(error_range) - 0.5)\n",
        "\n",
        "ax4.plot(error_range, squared_loss, label='Squared Loss (MSE)', linewidth=2)\n",
        "ax4.plot(error_range, absolute_loss, label='Absolute Loss (MAE)', linewidth=2)\n",
        "ax4.plot(error_range, huber_loss, label='Huber Loss', linewidth=2)\n",
        "ax4.set_xlabel('Error (y_true - y_pred)')\n",
        "ax4.set_ylabel('Loss')\n",
        "ax4.set_title('Comparison of Loss Functions')\n",
        "ax4.legend()\n",
        "ax4.grid(True, alpha=0.3)\n",
        "\n",
      ]
    }
  ]
}