{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89c6cf6a-4963-4cfb-8f80-a2c933830264",
   "metadata": {},
   "source": [
    "## Logistic Regression \n",
    "\n",
    "In **logistic regression**, we predict **probabilities** for binary classification:\n",
    "\n",
    "$$\n",
    "P(y_i = 1 \\mid X_i) = \\hat{y}_i = \\sigma(z_i) = \\frac{1}{1 + e^{-z_i}}, \\quad \\text{where } z_i = X_i^\\top \\beta\n",
    "$$\n",
    "\n",
    "This gives:\n",
    "\n",
    "* $\\hat{y}_i$ = predicted probability that $y_i = 1$\n",
    "* So naturally, $1 - \\hat{y}_i$ = predicted probability that $y_i = 0$\n",
    "\n",
    "---\n",
    "\n",
    "### Line 1: Defining the probability model\n",
    "\n",
    "$$\n",
    "P(y_i = 1 \\mid X_i) = \\hat{y}_i = \\sigma(z_i)\n",
    "$$\n",
    "\n",
    "* You're modeling the probability that the label $y_i = 1$ given the input $X_i$\n",
    "* Logistic regression outputs $\\hat{y}_i$, a number between 0 and 1 (a probability)\n",
    "* That’s done using the **sigmoid function** $\\sigma(z_i)$, which squashes the output of a linear model\n",
    "\n",
    "---\n",
    "\n",
    "### Line 2: Complement probability\n",
    "\n",
    "$$\n",
    "P(y_i = 0 \\mid X_i) = 1 - \\hat{y}_i\n",
    "$$\n",
    "\n",
    "* Since $y_i$ is binary (0 or 1), the only two possibilities are:\n",
    "\n",
    "  * $y_i = 1$ with probability $\\hat{y}_i$\n",
    "  * $y_i = 0$ with probability $1 - \\hat{y}_i$\n",
    "\n",
    "---\n",
    "\n",
    "### Line 3: General form for both 0 and 1\n",
    "\n",
    "$$\n",
    "P(y_i \\mid X_i) = \\hat{y}_i^{y_i} (1 - \\hat{y}_i)^{(1 - y_i)}\n",
    "$$\n",
    "\n",
    "This is the **core trick**.\n",
    "\n",
    "Why does this formula work?\n",
    "\n",
    "| Case    | $y_i$ | Becomes                                         | Interpretation |\n",
    "| ------- | ----- | ----------------------------------------------- | -------------- |\n",
    "| Class 1 | 1     | $\\hat{y}_i^1 (1 - \\hat{y}_i)^0 = \\hat{y}_i$     | correct        |\n",
    "| Class 0 | 0     | $\\hat{y}_i^0 (1 - \\hat{y}_i)^1 = 1 - \\hat{y}_i$ | correct        |\n",
    "\n",
    "So this single expression **works for both classes**.\n",
    "\n",
    "It says:\n",
    "\n",
    "> \"The probability of seeing label $y_i$ is equal to the predicted probability raised to the power of the actual label.\"\n",
    "\n",
    "---\n",
    "\n",
    "### Line 4: Likelihood of the whole dataset\n",
    "\n",
    "$$\n",
    "L(\\beta) = \\prod_{i=1}^{m} \\hat{y}_i^{y_i} (1 - \\hat{y}_i)^{(1 - y_i)}\n",
    "$$\n",
    "\n",
    "* For **independent samples**, the probability of observing the full dataset is the **product** of individual probabilities.\n",
    "* This is called the **likelihood function**.\n",
    "* It’s written as a function of the parameters $\\beta$, since we’re trying to choose the best $\\beta$ to make the data most likely.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f08245-f02e-4c92-bbb7-ba6fb0412d1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85419d62-30fa-4767-9385-8e7bba6ac26e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
