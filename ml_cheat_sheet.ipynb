{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8295e4c-f8a2-4a2e-aba6-ac1e292501bd",
   "metadata": {},
   "source": [
    "[May29,2025] \n",
    "# Machine Learning Cheat Sheet üìö\n",
    "\n",
    "*Revision Guide*\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Table of Contents\n",
    "1. [Introduction to Machine Learning](#introduction-to-machine-learning)\n",
    "2. [Mathematical Foundations](#mathematical-foundations)\n",
    "3. [Model Evaluation](#model-evaluation)\n",
    "4. [Regression Models](#regression-models)\n",
    "5. [Regularization](#regularization)\n",
    "6. [Dimensionality Reduction](#dimensionality-reduction)\n",
    "7. [Decision Trees & Ensemble Methods](#decision-trees--ensemble-methods)\n",
    "\n",
    "---\n",
    "\n",
    "## ü§ñ Introduction to Machine Learning\n",
    "\n",
    "### Key Definitions\n",
    "- **Machine Learning**: A subset of AI where algorithms learn patterns from data without explicit programming\n",
    "- **Algorithm**: A set of rules/instructions for solving problems\n",
    "- **Model**: The output of an algorithm trained on data\n",
    "- **Training**: Process of teaching the algorithm using historical data\n",
    "- **Prediction**: Using the trained model to make forecasts on new data\n",
    "\n",
    "### Types of Learning\n",
    "\n",
    "#### Supervised Learning\n",
    "- **Definition**: Learning from labeled data (input-output pairs)\n",
    "- **Goal**: Learn a mapping function from input to output\n",
    "- **Types**:\n",
    "  - **Classification**: Predicting categories/classes (discrete output)\n",
    "    - *Examples*: Email spam detection, image recognition, medical diagnosis\n",
    "  - **Regression**: Predicting continuous numerical values\n",
    "    - *Examples*: House price prediction, stock price forecasting, temperature prediction\n",
    "\n",
    "#### Unsupervised Learning\n",
    "- **Definition**: Finding hidden patterns in data without labels\n",
    "- **Types**:\n",
    "  - **Clustering**: Grouping similar data points\n",
    "    - *Examples*: Customer segmentation, gene sequencing, market research\n",
    "  - **Association**: Finding relationships between variables\n",
    "  - **Dimensionality Reduction**: Reducing number of features while preserving information\n",
    "\n",
    "### Training Process Components\n",
    "- **Dataset**: Collection of examples used for training\n",
    "- **Features**: Input variables (independent variables)\n",
    "- **Labels**: Output variables (dependent variables) - only in supervised learning\n",
    "- **Loss Function**: Measures how wrong the model's predictions are\n",
    "- **Training Algorithm**: Method to minimize the loss function\n",
    "\n",
    "---\n",
    "\n",
    "## üßÆ Mathematical Foundations\n",
    "\n",
    "### Linear Algebra Essentials\n",
    "\n",
    "#### Vectors\n",
    "```\n",
    "Vector: [x‚ÇÅ, x‚ÇÇ, ..., x‚Çô]\n",
    "```\n",
    "- **Dot Product**: a¬∑b = Œ£(a·µ¢ √ó b·µ¢)\n",
    "- **Vector Norm**: ||v|| = ‚àö(Œ£v·µ¢¬≤) (L2 norm)\n",
    "- **Unit Vector**: Vector with magnitude 1\n",
    "\n",
    "#### Matrices\n",
    "```\n",
    "Matrix A (m√ón):\n",
    "[a‚ÇÅ‚ÇÅ  a‚ÇÅ‚ÇÇ  ...  a‚ÇÅ‚Çô]\n",
    "[a‚ÇÇ‚ÇÅ  a‚ÇÇ‚ÇÇ  ...  a‚ÇÇ‚Çô]\n",
    "[...  ...  ...  ...]\n",
    "[a‚Çò‚ÇÅ  a‚Çò‚ÇÇ  ...  a‚Çò‚Çô]\n",
    "```\n",
    "\n",
    "**Key Operations**:\n",
    "- **Addition**: Element-wise addition\n",
    "- **Multiplication**: (A√óB)·µ¢‚±º = Œ£(A·µ¢‚Çñ √ó B‚Çñ‚±º)\n",
    "- **Transpose**: A·µÄ (flip rows and columns)\n",
    "- **Inverse**: A‚Åª¬π (exists only for square, non-singular matrices)\n",
    "- **Determinant**: Scalar value representing matrix properties\n",
    "\n",
    "**Important Concepts**:\n",
    "- **Eigenvalues (Œª)**: Av = Œªv (scaling factor)\n",
    "- **Eigenvectors (v)**: Direction that doesn't change under transformation\n",
    "- **SVD**: A = UŒ£V·µÄ (factorization into orthogonal matrices)\n",
    "- **Covariance Matrix**: Measures how variables vary together\n",
    "\n",
    "### Calculus & Optimization\n",
    "\n",
    "#### Derivatives\n",
    "- **Definition**: Rate of change of a function\n",
    "- **Partial Derivative**: ‚àÇf/‚àÇx (derivative with respect to one variable)\n",
    "- **Gradient**: ‚àáf = [‚àÇf/‚àÇx‚ÇÅ, ‚àÇf/‚àÇx‚ÇÇ, ..., ‚àÇf/‚àÇx‚Çô] (vector of all partial derivatives)\n",
    "- **Chain Rule**: d/dx[f(g(x))] = f'(g(x)) √ó g'(x)\n",
    "\n",
    "#### Optimization\n",
    "- **Gradient Descent**: x_{new} = x_{old} - Œ±‚àáf(x)\n",
    "  - Œ± = learning rate\n",
    "  - Move in opposite direction of gradient (steepest descent)\n",
    "- **Convex Function**: Has single global minimum (bowl-shaped)\n",
    "- **Non-Convex Function**: Multiple local minima (challenging to optimize)\n",
    "- **Learning Rate**: Step size in optimization\n",
    "  - Too high: May overshoot minimum\n",
    "  - Too low: Slow convergence\n",
    "\n",
    "### Probability & Statistics\n",
    "\n",
    "#### Probability Basics\n",
    "- **Sample Space**: Set of all possible outcomes\n",
    "- **Event**: Subset of sample space\n",
    "- **P(A)**: Probability of event A (0 ‚â§ P(A) ‚â§ 1)\n",
    "- **Conditional Probability**: P(A|B) = P(A‚à©B)/P(B)\n",
    "- **Independence**: P(A‚à©B) = P(A)√óP(B)\n",
    "\n",
    "#### Distributions\n",
    "- **Normal Distribution**: N(Œº, œÉ¬≤)\n",
    "  - Bell-shaped curve\n",
    "  - Parameters: mean (Œº), variance (œÉ¬≤)\n",
    "  - 68-95-99.7 rule\n",
    "\n",
    "#### Key Concepts\n",
    "- **Maximum Likelihood Estimation (MLE)**: Find parameters that maximize likelihood of observed data\n",
    "- **Entropy**: H(X) = -Œ£ P(x)log‚ÇÇP(x) (measure of uncertainty)\n",
    "- **Cross-Entropy**: Measures difference between two probability distributions\n",
    "- **Correlation**: Linear relationship strength (-1 to 1)\n",
    "- **Covariance**: How two variables change together\n",
    "\n",
    "---\n",
    "\n",
    "## üìä Model Evaluation\n",
    "\n",
    "### Train-Test Split Issues\n",
    "- **Data Leakage**: Information from future leaks into training\n",
    "- **Temporal Dependency**: Time-series data requires chronological splits\n",
    "- **Distribution Shift**: Training and test data from different distributions\n",
    "\n",
    "### Cross-Validation Techniques\n",
    "\n",
    "#### K-Fold Cross-Validation\n",
    "```\n",
    "Data split into K folds\n",
    "For each fold:\n",
    "  - Use as test set\n",
    "  - Train on remaining K-1 folds\n",
    "  - Calculate performance\n",
    "Average performance across all folds\n",
    "```\n",
    "\n",
    "#### Stratified K-Fold\n",
    "- Maintains class distribution in each fold\n",
    "- Important for imbalanced datasets\n",
    "\n",
    "#### Time Series CV\n",
    "- Respect temporal order\n",
    "- Use expanding or sliding window approach\n",
    "\n",
    "### Classification Metrics\n",
    "\n",
    "#### Confusion Matrix\n",
    "```\n",
    "                Predicted\n",
    "              Pos    Neg\n",
    "Actual  Pos   TP    FN\n",
    "        Neg   FP    TN\n",
    "```\n",
    "\n",
    "#### Performance Metrics\n",
    "- **Accuracy**: (TP + TN) / (TP + TN + FP + FN)\n",
    "- **Precision**: TP / (TP + FP) - \"Of predicted positives, how many are correct?\"\n",
    "- **Recall (Sensitivity)**: TP / (TP + FN) - \"Of actual positives, how many did we catch?\"\n",
    "- **Specificity**: TN / (TN + FP) - \"Of actual negatives, how many did we correctly identify?\"\n",
    "- **F1-Score**: 2 √ó (Precision √ó Recall) / (Precision + Recall)\n",
    "\n",
    "#### ROC Curve & AUC\n",
    "- **ROC**: Receiver Operating Characteristic (True Positive Rate vs False Positive Rate)\n",
    "- **AUC**: Area Under Curve (0.5 = random, 1.0 = perfect)\n",
    "- **Interpretation**: Probability that model ranks random positive higher than random negative\n",
    "\n",
    "### Regression Metrics\n",
    "- **MSE**: Mean Squared Error = Œ£(y·µ¢ - ≈∑·µ¢)¬≤/n\n",
    "- **RMSE**: Root Mean Squared Error = ‚àöMSE\n",
    "- **MAE**: Mean Absolute Error = Œ£|y·µ¢ - ≈∑·µ¢|/n\n",
    "- **R¬≤**: Coefficient of Determination = 1 - (SSres/SStot)\n",
    "\n",
    "### Bias-Variance Tradeoff\n",
    "- **Bias**: Error from oversimplifying assumptions\n",
    "  - High bias ‚Üí Underfitting\n",
    "- **Variance**: Error from sensitivity to training data fluctuations\n",
    "  - High variance ‚Üí Overfitting\n",
    "- **Total Error**: Bias¬≤ + Variance + Irreducible Error\n",
    "- **Sweet Spot**: Balance between bias and variance\n",
    "\n",
    "---\n",
    "\n",
    "## üìà Regression Models\n",
    "\n",
    "### Linear Regression\n",
    "\n",
    "#### Mathematical Formulation\n",
    "```\n",
    "y = Œ≤‚ÇÄ + Œ≤‚ÇÅx‚ÇÅ + Œ≤‚ÇÇx‚ÇÇ + ... + Œ≤‚Çôx‚Çô + Œµ\n",
    "```\n",
    "- **Œ≤‚ÇÄ**: Intercept\n",
    "- **Œ≤·µ¢**: Coefficients (slopes)\n",
    "- **Œµ**: Error term\n",
    "\n",
    "#### Cost Function (MSE)\n",
    "```\n",
    "J(Œ≤) = (1/2m) Œ£(hŒ≤(x‚ÅΩ‚Å±‚Åæ) - y‚ÅΩ‚Å±‚Åæ)¬≤\n",
    "```\n",
    "\n",
    "#### Normal Equation Solution\n",
    "```\n",
    "Œ≤ = (X·µÄX)‚Åª¬πX·µÄy\n",
    "```\n",
    "\n",
    "#### Key Properties\n",
    "- **Assumptions**:\n",
    "  - Linear relationship\n",
    "  - Independence of errors\n",
    "  - Homoscedasticity (constant variance)\n",
    "  - Normal distribution of errors\n",
    "- **Convex Optimization**: Single global minimum\n",
    "- **R-Squared Interpretation**: Proportion of variance explained\n",
    "\n",
    "### Logistic Regression\n",
    "\n",
    "#### Sigmoid Function\n",
    "```\n",
    "œÉ(z) = 1 / (1 + e‚Åª·∂ª)\n",
    "where z = Œ≤‚ÇÄ + Œ≤‚ÇÅx‚ÇÅ + ... + Œ≤‚Çôx‚Çô\n",
    "```\n",
    "\n",
    "#### Properties\n",
    "- **Output Range**: (0, 1) - represents probabilities\n",
    "- **Decision Boundary**: P(y=1) = 0.5 when z = 0\n",
    "- **Log-Odds**: ln(p/(1-p)) = z (linear relationship)\n",
    "\n",
    "#### Cost Function (Cross-Entropy)\n",
    "```\n",
    "J(Œ≤) = -(1/m) Œ£[y‚ÅΩ‚Å±‚Åælog(hŒ≤(x‚ÅΩ‚Å±‚Åæ)) + (1-y‚ÅΩ‚Å±‚Åæ)log(1-hŒ≤(x‚ÅΩ‚Å±‚Åæ))]\n",
    "```\n",
    "\n",
    "#### Maximum Likelihood Estimation\n",
    "- **Goal**: Find parameters that maximize likelihood of observed data\n",
    "- **Why Log-Likelihood**: Converts multiplication to addition, easier to optimize\n",
    "- **Convex Optimization**: Guaranteed global minimum\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Regularization\n",
    "\n",
    "### The Problem\n",
    "- **Overfitting**: Model learns training data too well, poor generalization\n",
    "- **High Variance**: Model sensitive to small changes in training data\n",
    "- **Complex Models**: Too many parameters relative to data\n",
    "\n",
    "### Regularization Techniques\n",
    "\n",
    "#### Ridge Regression (L2)\n",
    "```\n",
    "Cost = MSE + Œ± Œ£Œ≤·µ¢¬≤\n",
    "```\n",
    "- **Effect**: Shrinks coefficients toward zero\n",
    "- **Properties**: Keeps all features, reduces their impact\n",
    "- **Best for**: When all features are somewhat relevant\n",
    "\n",
    "#### Lasso Regression (L1)\n",
    "```\n",
    "Cost = MSE + Œ± Œ£|Œ≤·µ¢|\n",
    "```\n",
    "- **Effect**: Can set coefficients exactly to zero\n",
    "- **Properties**: Automatic feature selection\n",
    "- **Best for**: When only subset of features are relevant\n",
    "\n",
    "#### Elastic Net\n",
    "```\n",
    "Cost = MSE + Œ±‚ÇÅŒ£|Œ≤·µ¢| + Œ±‚ÇÇŒ£Œ≤·µ¢¬≤\n",
    "```\n",
    "- **Combines**: L1 and L2 penalties\n",
    "- **Benefits**: Feature selection + coefficient shrinkage\n",
    "- **Best for**: High-dimensional data with grouped features\n",
    "\n",
    "### Hyperparameter Œ± (Lambda)\n",
    "- **Œ± = 0**: No regularization (standard regression)\n",
    "- **Œ± ‚Üí ‚àû**: Maximum regularization (coefficients ‚Üí 0)\n",
    "- **Selection**: Use cross-validation to find optimal value\n",
    "\n",
    "---\n",
    "\n",
    "## üìâ Dimensionality Reduction\n",
    "\n",
    "### Principal Component Analysis (PCA)\n",
    "\n",
    "#### Purpose\n",
    "- **Reduce Dimensions**: Keep most important information\n",
    "- **Remove Redundancy**: Eliminate correlated features\n",
    "- **Visualization**: Project high-D data to 2D/3D\n",
    "- **Noise Reduction**: Focus on signal, reduce noise\n",
    "\n",
    "#### Mathematical Process\n",
    "1. **Standardize Data**: Zero mean, unit variance\n",
    "2. **Covariance Matrix**: C = (1/n)X·µÄX\n",
    "3. **Eigendecomposition**: C = PŒõP·µÄ\n",
    "4. **Select Components**: Choose top k eigenvectors\n",
    "5. **Transform Data**: Y = XP_k\n",
    "\n",
    "#### Key Concepts\n",
    "- **Principal Components**: Eigenvectors of covariance matrix\n",
    "- **Explained Variance**: Eigenvalues show information content\n",
    "- **Cumulative Explained Variance**: Choose k to retain 95% of variance\n",
    "- **Loading Scores**: How much each original feature contributes to PC\n",
    "\n",
    "#### Geometric Intuition\n",
    "- **First PC**: Direction of maximum variance\n",
    "- **Second PC**: Perpendicular to first, next highest variance\n",
    "- **Orthogonal**: All PCs are perpendicular to each other\n",
    "\n",
    "#### When to Use PCA\n",
    "- **High-dimensional data** with multicollinearity\n",
    "- **Preprocessing** before other algorithms\n",
    "- **Visualization** of complex datasets\n",
    "- **Storage/Speed** optimization\n",
    "\n",
    "---\n",
    "\n",
    "## üå≤ Decision Trees & Ensemble Methods\n",
    "\n",
    "### Decision Trees\n",
    "\n",
    "#### How They Work\n",
    "- **Recursive Splitting**: Divide data based on feature values\n",
    "- **Greedy Algorithm**: Choose best split at each node\n",
    "- **Tree Structure**: Root ‚Üí Internal Nodes ‚Üí Leaves\n",
    "\n",
    "#### Splitting Criteria\n",
    "\n",
    "##### Gini Impurity\n",
    "```\n",
    "Gini = 1 - Œ£p·µ¢¬≤\n",
    "```\n",
    "- **Range**: 0 (pure) to 0.5 (maximum impurity for binary)\n",
    "- **Interpretation**: Probability of misclassifying randomly chosen element\n",
    "\n",
    "##### Entropy\n",
    "```\n",
    "Entropy = -Œ£p·µ¢ log‚ÇÇ(p·µ¢)\n",
    "```\n",
    "- **Range**: 0 (pure) to log‚ÇÇ(classes) (maximum uncertainty)\n",
    "- **Interpretation**: Amount of information needed to classify\n",
    "\n",
    "##### Information Gain\n",
    "```\n",
    "IG = Entropy(parent) - Œ£(|child|/|parent|) √ó Entropy(child)\n",
    "```\n",
    "\n",
    "#### Advantages\n",
    "- **Interpretable**: Easy to understand and visualize\n",
    "- **No Assumptions**: Non-parametric, handles non-linear relationships\n",
    "- **Feature Selection**: Automatically selects relevant features\n",
    "- **Mixed Data Types**: Handles numerical and categorical features\n",
    "\n",
    "#### Disadvantages\n",
    "- **Overfitting**: Can create overly complex trees\n",
    "- **Instability**: Small data changes can create different trees\n",
    "- **Bias**: Favors features with more levels\n",
    "\n",
    "### Random Forest\n",
    "\n",
    "#### Concept\n",
    "- **Ensemble**: Combination of multiple decision trees\n",
    "- **Bagging**: Bootstrap Aggregating - train on random subsets\n",
    "- **Feature Randomness**: Consider random subset of features at each split\n",
    "- **Voting**: Majority vote (classification) or average (regression)\n",
    "\n",
    "#### Algorithm\n",
    "1. **Bootstrap Sampling**: Create B bootstrap samples\n",
    "2. **Train Trees**: Build tree on each sample with feature randomness\n",
    "3. **Aggregate**: Combine predictions from all trees\n",
    "\n",
    "#### Advantages\n",
    "- **Reduces Overfitting**: Averaging reduces variance\n",
    "- **Robust**: Less sensitive to outliers and noise\n",
    "- **Feature Importance**: Ranks feature importance\n",
    "- **Parallel**: Trees can be trained independently\n",
    "\n",
    "#### Hyperparameters\n",
    "- **n_estimators**: Number of trees\n",
    "- **max_features**: Features considered at each split\n",
    "- **max_depth**: Maximum tree depth\n",
    "- **min_samples_split**: Minimum samples to split node\n",
    "\n",
    "### Gradient Boosting\n",
    "\n",
    "#### Concept\n",
    "- **Sequential Learning**: Trees learn from previous trees' mistakes\n",
    "- **Additive Model**: F(x) = f‚ÇÅ(x) + f‚ÇÇ(x) + ... + f‚Çò(x)\n",
    "- **Gradient Descent**: Optimize loss function iteratively\n",
    "\n",
    "#### Algorithm\n",
    "1. **Initialize**: Start with simple prediction (mean)\n",
    "2. **Calculate Residuals**: Errors from current model\n",
    "3. **Train Tree**: Fit tree to residuals\n",
    "4. **Update Model**: Add tree with learning rate\n",
    "5. **Repeat**: Until convergence or max iterations\n",
    "\n",
    "#### Key Concepts\n",
    "- **Weak Learners**: Simple models (shallow trees)\n",
    "- **Learning Rate**: Controls contribution of each tree\n",
    "- **Regularization**: Prevent overfitting through tree constraints\n",
    "\n",
    "#### Popular Implementations\n",
    "- **XGBoost**: Extreme Gradient Boosting\n",
    "- **LightGBM**: Light Gradient Boosting Machine\n",
    "- **CatBoost**: Categorical Boosting\n",
    "\n",
    "---\n",
    "\n",
    "## üé§ Common Interview Questions\n",
    "\n",
    "### Conceptual Questions\n",
    "\n",
    "**Q: Explain the bias-variance tradeoff.**\n",
    "A: Bias is error from oversimplifying assumptions (underfitting), variance is error from sensitivity to training data (overfitting). Total error = bias¬≤ + variance + noise. Need to balance both for optimal performance.\n",
    "\n",
    "**Q: When would you use logistic regression vs. decision trees?**\n",
    "A: Logistic regression for linear relationships, interpretable coefficients, and probabilistic outputs. Decision trees for non-linear relationships, mixed data types, and when interpretability is crucial.\n",
    "\n",
    "**Q: How do you handle overfitting?**\n",
    "A: Regularization (L1/L2), cross-validation, more training data, feature selection, simpler models, early stopping, ensemble methods.\n",
    "\n",
    "**Q: Explain PCA in simple terms.**\n",
    "A: PCA finds the directions (principal components) along which data varies the most. It's like finding the best angle to photograph a 3D object in 2D while preserving maximum information.\n",
    "\n",
    "**Q: What's the difference between bagging and boosting?**\n",
    "A: Bagging trains models independently in parallel and averages results (reduces variance). Boosting trains models sequentially, each learning from previous mistakes (reduces bias).\n",
    "\n",
    "### Technical Questions\n",
    "\n",
    "**Q: How does gradient descent work?**\n",
    "A: Iteratively moves in the direction of steepest descent (negative gradient) to minimize cost function. Update rule: Œ∏ = Œ∏ - Œ±‚àáJ(Œ∏).\n",
    "\n",
    "**Q: What are eigenvalues and eigenvectors?**\n",
    "A: For matrix A, eigenvector v satisfies Av = Œªv, where Œª is eigenvalue. Eigenvector's direction doesn't change under transformation, only scaled by eigenvalue.\n",
    "\n",
    "**Q: Why use cross-entropy loss for classification?**\n",
    "A: Penalizes confident wrong predictions heavily, provides smooth gradients for optimization, and corresponds to maximum likelihood estimation for probabilistic models.\n",
    "\n",
    "**Q: How do you choose the number of principal components?**\n",
    "A: Plot cumulative explained variance, choose k where you retain 95% of variance, or use elbow method on scree plot.\n",
    "\n",
    "### Practical Questions\n",
    "\n",
    "**Q: How do you detect overfitting?**\n",
    "A: Large gap between training and validation performance, high variance in cross-validation scores, model performs well on training but poorly on test data.\n",
    "\n",
    "**Q: What's your approach to feature selection?**\n",
    "A: Domain knowledge, correlation analysis, univariate statistical tests, recursive feature elimination, regularization methods (L1), and feature importance from tree-based models.\n",
    "\n",
    "**Q: How do you handle imbalanced datasets?**\n",
    "A: Resampling (over/under-sampling), cost-sensitive learning, different evaluation metrics (precision, recall, F1), ensemble methods, and synthetic data generation (SMOTE).\n",
    "\n",
    "---\n",
    "\n",
    "## üìù Quick Reference Formulas\n",
    "\n",
    "### Linear Regression\n",
    "- **Prediction**: ≈∑ = XŒ≤\n",
    "- **Cost**: J = (1/2m)||XŒ≤ - y||¬≤\n",
    "- **Normal Equation**: Œ≤ = (X·µÄX)‚Åª¬πX·µÄy\n",
    "\n",
    "### Logistic Regression\n",
    "- **Sigmoid**: œÉ(z) = 1/(1 + e‚Åª·∂ª)\n",
    "- **Cost**: J = -Œ£[y log(ƒ•) + (1-y) log(1-ƒ•)]\n",
    "\n",
    "### Regularization\n",
    "- **Ridge**: J = MSE + Œ±||Œ≤||‚ÇÇ¬≤\n",
    "- **Lasso**: J = MSE + Œ±||Œ≤||‚ÇÅ\n",
    "\n",
    "### Evaluation Metrics\n",
    "- **Precision**: TP/(TP + FP)\n",
    "- **Recall**: TP/(TP + FN)\n",
    "- **F1**: 2 √ó (Precision √ó Recall)/(Precision + Recall)\n",
    "- **R¬≤**: 1 - SS_res/SS_tot\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f06c79-18a8-425c-87a6-5367f1ecf061",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
