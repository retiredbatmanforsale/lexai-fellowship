{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0190ed19-d02b-47ca-bb9e-8cf3d9e74c1c",
   "metadata": {},
   "source": [
    "### üß† Problem: Learn XOR-ish Behavior\n",
    "\n",
    "Let‚Äôs say you want a neural network to learn that:\n",
    "\n",
    "```\n",
    "Input x = [1, 0]\n",
    "Output y = 1\n",
    "```\n",
    "\n",
    "\n",
    "### üîß Neural Network Architecture\n",
    "\n",
    "* **Input layer**: 2 features\n",
    "* **Hidden layer**: 1 neuron (with sigmoid activation)\n",
    "* **Output layer**: 1 neuron (with sigmoid activation)\n",
    "\n",
    "\n",
    "### üìê Initialize Parameters\n",
    "\n",
    "Let‚Äôs assume:\n",
    "\n",
    "* **Input**:\n",
    "  $x = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$\n",
    "\n",
    "* **Target output**:\n",
    "  $y = 1$\n",
    "\n",
    "* **Weights and biases**:\n",
    "\n",
    "  * $W_1 = \\begin{bmatrix} 0.5 & -0.5 \\end{bmatrix}$ (1√ó2)\n",
    "  * $b_1 = 0$\n",
    "  * $W_2 = \\begin{bmatrix} 1 \\end{bmatrix}$ (1√ó1)\n",
    "  * $b_2 = 0$\n",
    "\n",
    "\n",
    "### ‚úÖ Step 1: Forward Pass\n",
    "\n",
    "#### Hidden Layer\n",
    "\n",
    "$$\n",
    "z_1 = W_1 \\cdot x + b_1 = (0.5)(1) + (-0.5)(0) + 0 = 0.5\n",
    "$$\n",
    "\n",
    "Apply **sigmoid** activation:\n",
    "\n",
    "$$\n",
    "a_1 = \\sigma(z_1) = \\frac{1}{1 + e^{-0.5}} ‚âà 0.622\n",
    "$$\n",
    "\n",
    "#### Output Layer\n",
    "\n",
    "$$\n",
    "z_2 = W_2 \\cdot a_1 + b_2 = (1)(0.622) + 0 = 0.622\n",
    "$$\n",
    "\n",
    "Again apply sigmoid:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\sigma(z_2) = \\frac{1}{1 + e^{-0.622}} ‚âà 0.650\n",
    "$$\n",
    "\n",
    "So, the **predicted output** is **0.65**.\n",
    "\n",
    "\n",
    "### üéØ Step 2: Compute Loss (Binary Cross Entropy)\n",
    "\n",
    "$$\n",
    "\\text{Loss} = -[y \\cdot \\log(\\hat{y}) + (1 - y) \\cdot \\log(1 - \\hat{y})]\n",
    "= -[\\log(0.65)] ‚âà 0.431\n",
    "$$\n",
    "\n",
    "\n",
    "### üîÅ Step 3: Backpropagation (Gradient Descent)\n",
    "\n",
    "We‚Äôll update weights using **gradient descent**. Let's do gradients step-by-step.\n",
    "\n",
    "#### a) Output Layer Gradients\n",
    "\n",
    "Let‚Äôs denote the derivative of loss w\\.r.t. prediction as:\n",
    "\n",
    "$$\n",
    "\\frac{dL}{d\\hat{y}} = -\\frac{y}{\\hat{y}} + \\frac{1 - y}{1 - \\hat{y}} = -\\frac{1}{0.65} ‚âà -1.538\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{d\\hat{y}}{dz_2} = \\hat{y}(1 - \\hat{y}) = 0.65(0.35) = 0.2275\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{dL}{dz_2} = \\frac{dL}{d\\hat{y}} \\cdot \\frac{d\\hat{y}}{dz_2} = -1.538 \\cdot 0.2275 ‚âà -0.35\n",
    "$$\n",
    "\n",
    "Now compute:\n",
    "\n",
    "* $\\frac{dL}{dW_2} = \\frac{dL}{dz_2} \\cdot a_1 = -0.35 \\cdot 0.622 ‚âà -0.218$\n",
    "* $\\frac{dL}{db_2} = \\frac{dL}{dz_2} = -0.35$\n",
    "\n",
    "#### b) Hidden Layer Gradients\n",
    "\n",
    "Use chain rule:\n",
    "\n",
    "$$\n",
    "\\frac{dL}{da_1} = \\frac{dL}{dz_2} \\cdot W_2 = -0.35 \\cdot 1 = -0.35\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{da_1}{dz_1} = a_1(1 - a_1) = 0.622(0.378) = 0.235\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{dL}{dz_1} = \\frac{dL}{da_1} \\cdot \\frac{da_1}{dz_1} = -0.35 \\cdot 0.235 ‚âà -0.082\n",
    "$$\n",
    "\n",
    "Now compute:\n",
    "\n",
    "* $\\frac{dL}{dW_1} = \\frac{dL}{dz_1} \\cdot x^T = -0.082 \\cdot \\begin{bmatrix} 1 & 0 \\end{bmatrix} = \\begin{bmatrix} -0.082 & 0 \\end{bmatrix}$\n",
    "* $\\frac{dL}{db_1} = -0.082$\n",
    "\n",
    "\n",
    "### üìâ Step 4: Update Weights\n",
    "\n",
    "Use a **learning rate** $\\eta = 0.1$\n",
    "\n",
    "* $W_2 := W_2 - \\eta \\cdot \\frac{dL}{dW_2} = 1 - 0.1 \\cdot (-0.218) ‚âà 1.022$\n",
    "* $b_2 := 0 - 0.1 \\cdot (-0.35) = 0.035$\n",
    "* $W_1 := \\begin{bmatrix} 0.5 & -0.5 \\end{bmatrix} - 0.1 \\cdot \\begin{bmatrix} -0.082 & 0 \\end{bmatrix} = \\begin{bmatrix} 0.5082 & -0.5 \\end{bmatrix}$\n",
    "* $b_1 := 0 - 0.1 \\cdot (-0.082) = 0.0082$\n",
    "\n",
    "\n",
    "### üîÅ Repeat\n",
    "\n",
    "You can now do another forward pass with updated weights to see the loss go down.\n",
    "\n",
    "\n",
    "What we have learnt so far: \n",
    "* Matrix multiplication\n",
    "* Activation (sigmoid)\n",
    "* Loss calculation\n",
    "* Backpropagation (chain rule)\n",
    "* Gradient descent\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1815954-450b-4f8a-bdc4-80d0b62601b5",
   "metadata": {},
   "source": [
    "## üß† Setup: One hidden layer neural network\n",
    "\n",
    "We‚Äôll assume this structure:\n",
    "\n",
    "* **Input**: $x \\in \\mathbb{R}^n$\n",
    "* **Hidden layer**:\n",
    "\n",
    "  * $z_1 = W_1 \\cdot x + b_1$\n",
    "  * $a_1 = \\sigma(z_1)$\n",
    "* **Output layer**:\n",
    "\n",
    "  * $z_2 = W_2 \\cdot a_1 + b_2$\n",
    "  * $\\hat{y} = \\sigma(z_2)$\n",
    "\n",
    "And loss:\n",
    "\n",
    "$$\n",
    "L = -[y \\log(\\hat{y}) + (1 - y) \\log(1 - \\hat{y})]\n",
    "$$\n",
    "\n",
    "We'll compute **4 gradients**:\n",
    "\n",
    "* $\\frac{\\partial L}{\\partial W_2}, \\frac{\\partial L}{\\partial b_2}$\n",
    "* $\\frac{\\partial L}{\\partial W_1}, \\frac{\\partial L}{\\partial b_1}$\n",
    "\n",
    "\n",
    "## ‚úÖ Step-by-step Gradient Flow\n",
    "\n",
    "\n",
    "### ‚ë† Output Layer Gradients\n",
    "\n",
    "**Start from the loss and go backward**.\n",
    "\n",
    "Let‚Äôs define:\n",
    "\n",
    "* $\\hat{y} = \\sigma(z_2)$\n",
    "* $\\sigma'(z_2) = \\hat{y}(1 - \\hat{y})$\n",
    "\n",
    "#### a) $\\frac{\\partial L}{\\partial z_2}$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial z_2} = (\\hat{y} - y)\n",
    "$$\n",
    "\n",
    "This is simple! For binary cross-entropy + sigmoid, this is the derivative.\n",
    "\n",
    "#### b) Gradients for W‚ÇÇ and b‚ÇÇ:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W_2} = \\frac{\\partial L}{\\partial z_2} \\cdot a_1^T\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial b_2} = \\frac{\\partial L}{\\partial z_2}\n",
    "$$\n",
    "\n",
    "üß† So far:\n",
    "\n",
    "* You got $\\delta_2 = \\hat{y} - y$\n",
    "* Multiply by $a_1$ to get $\\frac{dL}{dW_2}$\n",
    "\n",
    "\n",
    "### ‚ë° Hidden Layer Gradients\n",
    "\n",
    "We now pass the gradient back into the hidden layer.\n",
    "\n",
    "#### a) Compute error term for hidden layer:\n",
    "\n",
    "$$\n",
    "\\delta_1 = (W_2^T \\cdot \\delta_2) \\cdot \\sigma'(z_1)\n",
    "$$\n",
    "\n",
    "* $W_2^T \\cdot \\delta_2$ is how the output layer‚Äôs gradient flows backward.\n",
    "* $\\sigma'(z_1) = a_1(1 - a_1)$\n",
    "\n",
    "So:\n",
    "\n",
    "$$\n",
    "\\delta_1 = \\text{error flowing into hidden layer}\n",
    "$$\n",
    "\n",
    "#### b) Gradients for W‚ÇÅ and b‚ÇÅ:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W_1} = \\delta_1 \\cdot x^T\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial b_1} = \\delta_1\n",
    "$$\n",
    "\n",
    "\n",
    "## üéØ Summary of Backpropagation Steps\n",
    "\n",
    "From forward pass:\n",
    "\n",
    "* $z_1 = W_1 x + b_1$\n",
    "* $a_1 = \\sigma(z_1)$\n",
    "* $z_2 = W_2 a_1 + b_2$\n",
    "* $\\hat{y} = \\sigma(z_2)$\n",
    "\n",
    "Now backward pass:\n",
    "\n",
    "```text\n",
    "Step 1: Œ¥‚ÇÇ = (≈∑ - y)\n",
    "\n",
    "Step 2:\n",
    "‚àÇL/‚àÇW‚ÇÇ = Œ¥‚ÇÇ √ó a‚ÇÅ·µÄ\n",
    "‚àÇL/‚àÇb‚ÇÇ = Œ¥‚ÇÇ\n",
    "\n",
    "Step 3: Œ¥‚ÇÅ = (W‚ÇÇ·µÄ √ó Œ¥‚ÇÇ) * œÉ'(z‚ÇÅ)\n",
    "\n",
    "Step 4:\n",
    "‚àÇL/‚àÇW‚ÇÅ = Œ¥‚ÇÅ √ó x·µÄ\n",
    "‚àÇL/‚àÇb‚ÇÅ = Œ¥‚ÇÅ\n",
    "```\n",
    "\n",
    "This is the complete core of backpropagation for a 1-hidden-layer network. No harder than this.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fbef34-59ab-49bb-95c2-a1331d64703c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
