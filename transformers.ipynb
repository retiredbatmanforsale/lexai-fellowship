{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38bf80f8-7cc9-495f-8aa3-b464079e95ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TRANSFORMER COMPONENTS TUTORIAL\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "# Let's start by understanding each component step by step\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TRANSFORMER COMPONENTS TUTORIAL\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "466eab92-3d1f-46e2-ba95-d1fe6d6544c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. POSITIONAL ENCODING\n",
    "# ======================\n",
    "# Problem: Unlike RNNs, Transformers process all positions simultaneously\n",
    "# Solution: Add positional information to input embeddings\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Adds positional information to word embeddings.\n",
    "    Uses sine and cosine functions of different frequencies.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Create a matrix to hold positional encodings\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        \n",
    "        # Create division term for the encoding formula\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n",
    "                           -(math.log(10000.0) / d_model))\n",
    "        \n",
    "        # Apply sine to even indices\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        # Apply cosine to odd indices  \n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        # Add batch dimension and register as buffer (not a parameter)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Add positional encoding to input embeddings\n",
    "        return x + self.pe[:, :x.size(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d86a8a-f0be-4abc-af81-0012b219b1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. SELF-ATTENTION MECHANISM\n",
    "# ===========================\n",
    "# Key insight: Instead of processing sequentially like RNNs,\n",
    "# allow each position to attend to ALL positions simultaneously\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Self-attention allows each position to attend to all positions.\n",
    "    Think of it as: \"For each word, how much should I focus on every other word?\"\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Linear projections for Query, Key, Value\n",
    "        self.W_q = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_k = nn.Linear(d_model, d_model, bias=False) \n",
    "        self.W_v = nn.Linear(d_model, d_model, bias=False)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, seq_len, d_model = x.size()\n",
    "        \n",
    "        # Step 1: Create Query, Key, Value matrices\n",
    "        Q = self.W_q(x)  # What am I looking for?\n",
    "        K = self.W_k(x)  # What do I contain?\n",
    "        V = self.W_v(x)  # What is my actual content?\n",
    "        \n",
    "        # Step 2: Compute attention scores\n",
    "        # scores[i,j] = how much position i should attend to position j\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_model)\n",
    "        \n",
    "        # Step 3: Apply mask if provided (for decoder)\n",
    "        if mask is not None:\n",
    "            scores.masked_fill_(mask == 0, -1e9)\n",
    "        \n",
    "        # Step 4: Apply softmax to get attention weights\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        \n",
    "        # Step 5: Apply attention weights to values\n",
    "        output = torch.matmul(attention_weights, V)\n",
    "        \n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd36965b-0520-43c7-96dc-879c4a3ee48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. MULTI-HEAD ATTENTION\n",
    "# =======================\n",
    "# Instead of one attention, use multiple \"heads\" to capture different relationships\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-head attention runs several attention mechanisms in parallel.\n",
    "    Each head can focus on different types of relationships.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_model // n_heads  # dimension of each head\n",
    "        \n",
    "        # Linear layers for all heads combined\n",
    "        self.W_q = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_k = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_v = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_o = nn.Linear(d_model, d_model)  # Output projection\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, seq_len, d_model = x.size()\n",
    "        \n",
    "        # Step 1: Linear projections in batch from d_model => h x d_k\n",
    "        Q = self.W_q(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.W_k(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.W_v(x).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # Step 2: Apply attention on all the projected vectors in batch\n",
    "        attention_output, attention_weights = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        \n",
    "        # Step 3: Concatenate heads and put through final linear layer\n",
    "        attention_output = attention_output.transpose(1, 2).contiguous().view(\n",
    "            batch_size, seq_len, d_model)\n",
    "        \n",
    "        return self.W_o(attention_output)\n",
    "    \n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores.masked_fill_(mask == 0, -1e9)\n",
    "            \n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        output = torch.matmul(attention_weights, V)\n",
    "        \n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1249cbdf-cc24-48af-be38-89ce8f4b515c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. LAYER NORMALIZATION\n",
    "# ======================\n",
    "# Normalizes inputs to each layer for stable training\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    \"\"\"\n",
    "    Layer normalization normalizes across the feature dimension.\n",
    "    Helps with training stability and convergence.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(d_model))  # Scale parameter\n",
    "        self.beta = nn.Parameter(torch.zeros(d_model))  # Shift parameter\n",
    "        self.eps = eps\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.gamma * (x - mean) / (std + self.eps) + self.beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67088653-930a-4881-af86-5b07b68d6f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. FEED FORWARD NETWORK\n",
    "# =======================\n",
    "# Position-wise fully connected network applied to each position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca6faaf-0cda-48d8-a967-b56a6e766b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    Position-wise feed-forward network.\n",
    "    Applied to each position separately and identically.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear2(self.dropout(F.relu(self.linear1(x))))\n",
    "\n",
    "# 6. ENCODER BLOCK\n",
    "# ================\n",
    "# Combines multi-head attention + feed forward with residual connections\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder block: Multi-Head Attention + Feed Forward\n",
    "    Each sub-layer has residual connection + layer norm\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.multi_head_attention = MultiHeadAttention(d_model, n_heads)\n",
    "        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n",
    "        self.norm1 = LayerNorm(d_model)\n",
    "        self.norm2 = LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        # Multi-head attention with residual connection\n",
    "        attn_output = self.multi_head_attention(x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))  # Residual connection\n",
    "        \n",
    "        # Feed forward with residual connection\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))  # Residual connection\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89adc50c-a499-4c42-9e0b-256ac50f2097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. DECODER BLOCK\n",
    "# ================\n",
    "# Like encoder but with masked self-attention + encoder-decoder attention\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder block: Masked Multi-Head Attention + Encoder-Decoder Attention + Feed Forward\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.masked_multi_head_attention = MultiHeadAttention(d_model, n_heads)\n",
    "        self.encoder_decoder_attention = MultiHeadAttention(d_model, n_heads)\n",
    "        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n",
    "        self.norm1 = LayerNorm(d_model)\n",
    "        self.norm2 = LayerNorm(d_model)\n",
    "        self.norm3 = LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, encoder_output, src_mask=None, tgt_mask=None):\n",
    "        # Masked self-attention (can only attend to previous positions)\n",
    "        attn_output = self.masked_multi_head_attention(x, tgt_mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        \n",
    "        # Encoder-decoder attention (attend to encoder output)\n",
    "        attn_output = self.encoder_decoder_attention(x, src_mask)  # Simplified for demo\n",
    "        x = self.norm2(x + self.dropout(attn_output))\n",
    "        \n",
    "        # Feed forward\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm3(x + self.dropout(ff_output))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35cc9e1f-0a3a-4616-8124-0a9c684463d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. COMPLETE TRANSFORMER MODEL\n",
    "# =============================\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete Transformer model combining all components\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, d_model=512, n_heads=8, n_layers=6, d_ff=2048, \n",
    "                 max_len=5000, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Embedding layers\n",
    "        self.src_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.tgt_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_len)\n",
    "        \n",
    "        # Encoder and Decoder stacks\n",
    "        self.encoder_blocks = nn.ModuleList([\n",
    "            EncoderBlock(d_model, n_heads, d_ff, dropout) for _ in range(n_layers)\n",
    "        ])\n",
    "        self.decoder_blocks = nn.ModuleList([\n",
    "            DecoderBlock(d_model, n_heads, d_ff, dropout) for _ in range(n_layers)\n",
    "        ])\n",
    "        \n",
    "        # Output projection\n",
    "        self.output_projection = nn.Linear(d_model, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n",
    "        # Encoder\n",
    "        src_embedded = self.dropout(self.positional_encoding(\n",
    "            self.src_embedding(src) * math.sqrt(self.d_model)))\n",
    "        \n",
    "        encoder_output = src_embedded\n",
    "        for encoder_block in self.encoder_blocks:\n",
    "            encoder_output = encoder_block(encoder_output, src_mask)\n",
    "        \n",
    "        # Decoder  \n",
    "        tgt_embedded = self.dropout(self.positional_encoding(\n",
    "            self.tgt_embedding(tgt) * math.sqrt(self.d_model)))\n",
    "        \n",
    "        decoder_output = tgt_embedded\n",
    "        for decoder_block in self.decoder_blocks:\n",
    "            decoder_output = decoder_block(decoder_output, encoder_output, \n",
    "                                         src_mask, tgt_mask)\n",
    "        \n",
    "        # Output projection\n",
    "        output = self.output_projection(decoder_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4e03f5-a568-45d1-9e8c-b74206af0ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. DEMONSTRATION AND TESTING\n",
    "# ============================\n",
    "\n",
    "def create_padding_mask(seq, pad_idx=0):\n",
    "    \"\"\"Create mask to ignore padding tokens\"\"\"\n",
    "    return (seq != pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "def create_look_ahead_mask(size):\n",
    "    \"\"\"Create mask to prevent attending to future positions\"\"\"\n",
    "    mask = torch.triu(torch.ones(size, size), diagonal=1)\n",
    "    return mask == 0\n",
    "\n",
    "print(\"\\n1. TESTING POSITIONAL ENCODING\")\n",
    "print(\"-\" * 40)\n",
    "pos_enc = PositionalEncoding(d_model=512, max_len=100)\n",
    "# Test with dummy input\n",
    "dummy_input = torch.randn(1, 10, 512)  # batch_size=1, seq_len=10, d_model=512\n",
    "pos_encoded = pos_enc(dummy_input)\n",
    "print(f\"Input shape: {dummy_input.shape}\")\n",
    "print(f\"After positional encoding: {pos_encoded.shape}\")\n",
    "print(\"âœ“ Positional encoding adds location information to embeddings\")\n",
    "\n",
    "print(\"\\n2. TESTING SELF-ATTENTION\")\n",
    "print(\"-\" * 40)\n",
    "self_attn = SelfAttention(d_model=512)\n",
    "attn_output, attn_weights = self_attn(dummy_input)\n",
    "print(f\"Attention output shape: {attn_output.shape}\")\n",
    "print(f\"Attention weights shape: {attn_weights.shape}\")\n",
    "print(\"âœ“ Self-attention allows each position to attend to all positions\")\n",
    "\n",
    "print(\"\\n3. TESTING MULTI-HEAD ATTENTION\")\n",
    "print(\"-\" * 40)\n",
    "multi_head_attn = MultiHeadAttention(d_model=512, n_heads=8)\n",
    "mh_output = multi_head_attn(dummy_input)\n",
    "print(f\"Multi-head attention output shape: {mh_output.shape}\")\n",
    "print(\"âœ“ Multi-head attention captures different types of relationships\")\n",
    "\n",
    "print(\"\\n4. TESTING ENCODER BLOCK\")\n",
    "print(\"-\" * 40)\n",
    "encoder_block = EncoderBlock(d_model=512, n_heads=8, d_ff=2048)\n",
    "encoder_output = encoder_block(dummy_input)\n",
    "print(f\"Encoder block output shape: {encoder_output.shape}\")\n",
    "print(\"âœ“ Encoder block combines attention + feed-forward with residual connections\")\n",
    "\n",
    "print(\"\\n5. TESTING COMPLETE TRANSFORMER\")\n",
    "print(\"-\" * 40)\n",
    "vocab_size = 10000\n",
    "transformer = Transformer(vocab_size=vocab_size, d_model=512, n_heads=8, n_layers=6)\n",
    "\n",
    "# Create dummy sequences\n",
    "src_seq = torch.randint(1, vocab_size, (2, 10))  # batch_size=2, seq_len=10\n",
    "tgt_seq = torch.randint(1, vocab_size, (2, 8))   # batch_size=2, seq_len=8\n",
    "\n",
    "# Create masks\n",
    "src_mask = create_padding_mask(src_seq)\n",
    "tgt_mask = create_look_ahead_mask(8).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "print(f\"Source sequence shape: {src_seq.shape}\")\n",
    "print(f\"Target sequence shape: {tgt_seq.shape}\")\n",
    "print(f\"Source mask shape: {src_mask.shape}\")\n",
    "print(f\"Target mask shape: {tgt_mask.shape}\")\n",
    "\n",
    "# Forward pass\n",
    "output = transformer(src_seq, tgt_seq, src_mask, tgt_mask)\n",
    "print(f\"Transformer output shape: {output.shape}\")\n",
    "print(\"âœ“ Complete transformer processes input successfully!\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"KEY CONCEPTS SUMMARY:\")\n",
    "print(\"=\" * 60)\n",
    "print(\"ðŸ”¸ POSITIONAL ENCODING: Adds position information since Transformers lack inherent sequence order\")\n",
    "print(\"ðŸ”¸ SELF-ATTENTION: Each position attends to all positions simultaneously\")  \n",
    "print(\"ðŸ”¸ MULTI-HEAD ATTENTION: Multiple attention mechanisms capture different relationships\")\n",
    "print(\"ðŸ”¸ RESIDUAL CONNECTIONS: Help gradients flow and enable deeper networks\")\n",
    "print(\"ðŸ”¸ LAYER NORMALIZATION: Stabilizes training by normalizing layer inputs\")\n",
    "print(\"ðŸ”¸ ENCODER-DECODER: Encoder processes input, decoder generates output autoregressively\")\n",
    "print(\"ðŸ”¸ MASKS: Prevent attention to padding tokens and future positions in decoder\")\n",
    "\n",
    "print(\"\\nðŸš€ You now understand the core components of Transformers!\")\n",
    "print(\"Each component serves a specific purpose in enabling the model to process\")\n",
    "print(\"sequences in parallel while capturing complex relationships between positions.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
