{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30a56c2b-9997-4b0c-8b82-2eca5d0225c3",
   "metadata": {},
   "source": [
    "We are looking at how to compute the derivatives of the **cost function** for linear regression. The cost function is:\n",
    "\n",
    "$$\n",
    "J(m, c) = \\sum_{i=1}^n \\left( y_i - (mx_i + c) \\right)^2\n",
    "$$\n",
    "\n",
    "\n",
    "### What are we trying to do?\n",
    "\n",
    "We are trying to **minimize** the total error between our predicted line $\\hat{y}_i = mx_i + c$ and the actual true values $y_i$. The total error is the **sum of squared differences** (residuals).\n",
    "\n",
    "To do this, we want to find values of $m$ (slope) and $c$ (intercept) such that this error is the smallest. In math, we do this by computing the **derivatives**—which tell us the direction in which the function is increasing or decreasing.\n",
    "\n",
    "\n",
    "\n",
    "### Derivative Intuition\n",
    "\n",
    "Imagine you are walking on a hilly surface (the graph of the function). The **derivative** tells you the slope of the hill at your feet. If the slope is steeply upwards, go the other way! If it's steeply downwards, keep going—it means you're minimizing the function.\n",
    "\n",
    "\n",
    "### Let's write the cost function again:\n",
    "\n",
    "$$\n",
    "J(m, c) = \\sum_{i=1}^n \\left( y_i - (mx_i + c) \\right)^2\n",
    "$$\n",
    "\n",
    "Let’s simplify this inner expression a bit. For each data point:\n",
    "\n",
    "$$\n",
    "\\text{error}_i = y_i - (mx_i + c)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{error}_i^2 = \\left( y_i - (mx_i + c) \\right)^2\n",
    "$$\n",
    "\n",
    "So our goal is to **adjust** $m$ and $c$ to reduce the total squared error.\n",
    "\n",
    "---\n",
    "\n",
    "## Derivative with respect to **m** (slope)\n",
    "\n",
    "We're going to ask: “How does the error change if we nudge the slope $m$ a little bit?”\n",
    "\n",
    "Let’s derive $\\frac{\\partial J}{\\partial m}$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial m} = \\sum_{i=1}^n 2 \\cdot \\left( y_i - (mx_i + c) \\right) \\cdot (-x_i)\n",
    "$$\n",
    "\n",
    "Why this form?\n",
    "\n",
    "* $2 \\cdot \\text{error}$: Comes from squaring a value (recall: $d/dx[x^2] = 2x$)\n",
    "* $-x_i$: Comes from chain rule; you're changing $m$, and the inside has $-mx_i$, which gives derivative $-x_i$\n",
    "\n",
    "So, putting it all together:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial m} = -2 \\sum_{i=1}^n x_i \\cdot \\left( y_i - (mx_i + c) \\right)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Derivative with respect to **c** (intercept)\n",
    "\n",
    "Same logic, but now we ask: “What if we nudge the intercept $c$?”\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial c} = \\sum_{i=1}^n 2 \\cdot \\left( y_i - (mx_i + c) \\right) \\cdot (-1)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial c} = -2 \\sum_{i=1}^n \\left( y_i - (mx_i + c) \\right)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Final Derivatives\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial m} = -2 \\sum_{i=1}^n x_i \\cdot \\left( y_i - (mx_i + c) \\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial c} = -2 \\sum_{i=1}^n \\left( y_i - (mx_i + c) \\right)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### What do you do with these?\n",
    "\n",
    "You use them in **gradient descent**:\n",
    "\n",
    "* Repeatedly update:\n",
    "\n",
    "  $$\n",
    "  m := m - \\alpha \\cdot \\frac{\\partial J}{\\partial m}\n",
    "  $$\n",
    "\n",
    "  $$\n",
    "  c := c - \\alpha \\cdot \\frac{\\partial J}{\\partial c}\n",
    "  $$\n",
    "* Where $\\alpha$ is the **learning rate** (a small number like 0.01)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c246c024-5fa3-4afa-b04a-fbf768a6163d",
   "metadata": {},
   "source": [
    "### 1. **Partial Derivatives (Multivariable Functions)**\n",
    "\n",
    "When you have a function of **multiple variables**, like:\n",
    "\n",
    "$$\n",
    "f(x, y) = x^2 + y^2\n",
    "$$\n",
    "\n",
    "A **partial derivative** measures the rate of change of the function **with respect to one variable**, while **keeping all others constant**.\n",
    "\n",
    "* $\\frac{\\partial f}{\\partial x} = 2x$\n",
    "* $\\frac{\\partial f}{\\partial y} = 2y$\n",
    "\n",
    "So partial derivatives are useful in **multivariable calculus** and optimization.\n",
    "\n",
    "### 2. **Total Derivatives (Single Variable or Chain Rule Context)**\n",
    "\n",
    "A **total derivative** considers the full dependency of the function on all variables — including **indirect dependencies** via the chain rule.\n",
    "\n",
    "For example, if:\n",
    "\n",
    "* $z = f(x, y)$, and\n",
    "* $x = x(t), y = y(t)$, then\n",
    "\n",
    "$$\n",
    "\\frac{dz}{dt} = \\frac{\\partial f}{\\partial x} \\frac{dx}{dt} + \\frac{\\partial f}{\\partial y} \\frac{dy}{dt}\n",
    "$$\n",
    "\n",
    "This is the **total derivative** of $z$ with respect to $t$.\n",
    "\n",
    "\n",
    "| Type                | Function Type         | Meaning                                                   |\n",
    "| ------------------- | --------------------- | --------------------------------------------------------- |\n",
    "| Partial Derivative  | Multivariable         | Change in function wrt one variable, keeping others fixed |\n",
    "| Total Derivative    | Function of functions | Overall rate of change considering all dependencies       |\n",
    "| Ordinary Derivative | Single-variable       | Standard derivative (like $f'(x)$)                        |\n",
    "\n",
    "---\n",
    "\n",
    "In **convex optimization**, you typically deal with **partial derivatives**, and collectively they form the **gradient vector**:\n",
    "\n",
    "$$\n",
    "\\nabla f(x) = \\begin{bmatrix}\n",
    "\\frac{\\partial f}{\\partial x_1} \\\\\n",
    "\\frac{\\partial f}{\\partial x_2} \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial f}{\\partial x_n}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12233465-3caf-4a2e-ad7c-1274c3e2e237",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4f19eab0-a6bc-4b6e-97ed-dec1eec6ec59",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940eed75-f1dd-4427-adce-0b090e39e00e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
