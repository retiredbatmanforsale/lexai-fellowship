# Machine Learning Data Preprocessing: A Comprehensive Guide
# Author: Google Software Engineer
# Target: Engineering Students

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import make_classification
from sklearn.impute import SimpleImputer, KNNImputer
from sklearn.preprocessing import StandardScaler, PowerTransformer
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.utils import resample
from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler
import warnings
warnings.filterwarnings('ignore')

# Set style for better plots
plt.style.use('seaborn-v0_8')
sns.set_palette("husl")

print("üöÄ Machine Learning Data Preprocessing Tutorial")
print("=" * 50)

# =============================================================================
# 1. IDENTIFYING AND HANDLING MISSING VALUES
# =============================================================================

print("\nüìä SECTION 1: IDENTIFYING AND HANDLING MISSING VALUES")
print("-" * 55)

# Create a sample dataset with missing values
np.random.seed(42)
data = {
    'age': [25, 30, np.nan, 35, 28, np.nan, 45, 32, 29, 38],
    'salary': [50000, 60000, 55000, np.nan, 52000, 58000, np.nan, 61000, 49000, 67000],
    'experience': [2, 5, 3, np.nan, 1, 4, 8, 6, 2, 9],
    'department': ['IT', 'HR', None, 'IT', 'Finance', 'HR', 'IT', None, 'Finance', 'IT'],
    'performance_score': [8.5, 7.2, 9.1, 6.8, 8.0, 7.5, 9.3, 8.2, 7.8, 9.0]
}

df = pd.DataFrame(data)
print("üìã Original Dataset:")
print(df)

print("\nüîç 1.1 Identifying Missing Values:")
print("Basic info about missing values:")
print(f"Total missing values: {df.isnull().sum().sum()}")
print("\nMissing values per column:")
print(df.isnull().sum())

print("\nMissing values percentage:")
missing_percent = (df.isnull().sum() / len(df)) * 100
print(missing_percent)

# Visual representation of missing values
plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
sns.heatmap(df.isnull(), cbar=True, yticklabels=False, cmap='viridis')
plt.title('Missing Values Heatmap')

plt.subplot(1, 2, 2)
missing_percent.plot(kind='bar')
plt.title('Missing Values Percentage by Column')
plt.ylabel('Percentage')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

print("\nüõ†Ô∏è 1.2 Different Ways to Handle Missing Values:")

# Method 1: Drop rows with any missing values
df_dropped_rows = df.dropna()
print(f"After dropping rows: {len(df_dropped_rows)} rows remaining (from {len(df)})")

# Method 2: Drop columns with missing values
df_dropped_cols = df.dropna(axis=1)
print(f"Columns remaining after dropping: {list(df_dropped_cols.columns)}")

# Method 3: Drop based on threshold
df_threshold = df.dropna(thresh=4)  # Keep rows with at least 4 non-null values
print(f"After threshold dropping: {len(df_threshold)} rows remaining")

# =============================================================================
# 2. IMPUTING MISSING VALUES WITH DIFFERENT STRATEGIES
# =============================================================================

print("\nüîß SECTION 2: IMPUTING MISSING VALUES")
print("-" * 40)

# Separate numerical and categorical columns
numerical_cols = ['age', 'salary', 'experience', 'performance_score']
categorical_cols = ['department']

print("üìà 2.1 Numerical Imputation Strategies:")

# Mean Imputation
mean_imputer = SimpleImputer(strategy='mean')
df_mean = df.copy()
df_mean[numerical_cols] = mean_imputer.fit_transform(df[numerical_cols])
print("\nMean Imputation Results:")
print(df_mean[numerical_cols].head())

# Median Imputation
median_imputer = SimpleImputer(strategy='median')
df_median = df.copy()
df_median[numerical_cols] = median_imputer.fit_transform(df[numerical_cols])
print("\nMedian Imputation Results:")
print(df_median[numerical_cols].head())

# Mode Imputation for categorical
mode_imputer = SimpleImputer(strategy='most_frequent')
df_mode = df.copy()
df_mode[categorical_cols] = mode_imputer.fit_transform(df[categorical_cols])
print("\nMode Imputation for Categorical:")
print(df_mode['department'].value_counts())

# KNN Imputation
print("\nüéØ 2.2 KNN Imputation:")
knn_imputer = KNNImputer(n_neighbors=3)
df_knn = df.copy()
# KNN imputation for numerical columns only
df_knn[numerical_cols] = knn_imputer.fit_transform(df[numerical_cols])
print("KNN Imputation Results:")
print(df_knn[numerical_cols].head())

# Forward Fill and Backward Fill
df_ffill = df.fillna(method='ffill')
df_bfill = df.fillna(method='bfill')

# Comparison of different imputation methods
plt.figure(figsize=(15, 10))
methods = ['Original', 'Mean', 'Median', 'KNN']
datasets = [df, df_mean, df_median, df_knn]

for i, (method, data) in enumerate(zip(methods, datasets)):
    plt.subplot(2, 2, i+1)
    data['age'].hist(bins=10, alpha=0.7)
    plt.title(f'Age Distribution - {method}')
    plt.xlabel('Age')
    plt.ylabel('Frequency')

plt.tight_layout()
plt.show()

# =============================================================================
# 3. LIMITATIONS OF DIFFERENT IMPUTATION STRATEGIES
# =============================================================================

print("\n‚ö†Ô∏è SECTION 3: LIMITATIONS OF IMPUTATION STRATEGIES")
print("-" * 50)

# Create a more complex dataset to demonstrate limitations
np.random.seed(42)
n_samples = 1000

# Create correlated data
age = np.random.normal(35, 10, n_samples)
salary = 30000 + age * 1000 + np.random.normal(0, 5000, n_samples)
experience = np.maximum(0, age - 22 + np.random.normal(0, 2, n_samples))

# Introduce MCAR (Missing Completely At Random) missingness
mcar_mask = np.random.random(n_samples) < 0.1
age_mcar = age.copy()
age_mcar[mcar_mask] = np.nan

# Introduce MAR (Missing At Random) missingness - higher salary people less likely to report
mar_prob = 1 / (1 + np.exp((salary - 60000) / 10000))  # Sigmoid function
mar_mask = np.random.random(n_samples) < mar_prob * 0.2
salary_mar = salary.copy()
salary_mar[mar_mask] = np.nan

complex_df = pd.DataFrame({
    'age': age_mcar,
    'salary': salary_mar,
    'experience': experience
})

# Add some missing values to experience too
exp_missing = np.random.random(n_samples) < 0.05
complex_df.loc[exp_missing, 'experience'] = np.nan

print("üîç 3.1 Analyzing Imputation Method Performance:")

# Compare different imputation methods on correlated data
imputation_methods = {
    'Mean': SimpleImputer(strategy='mean'),
    'Median': SimpleImputer(strategy='median'),
    'KNN': KNNImputer(n_neighbors=5)
}

results = {}
original_corr = np.corrcoef(age, salary)[0, 1]
print(f"Original correlation between age and salary: {original_corr:.3f}")

for method_name, imputer in imputation_methods.items():
    imputed_data = imputer.fit_transform(complex_df[['age', 'salary']])
    imputed_corr = np.corrcoef(imputed_data[:, 0], imputed_data[:, 1])[0, 1]
    results[method_name] = imputed_corr
    print(f"{method_name} imputation correlation: {imputed_corr:.3f}")

print("\nüìä 3.2 Key Limitations:")
limitations = {
    "Mean/Median Imputation": [
        "Reduces variance in the data",
        "Doesn't preserve relationships between variables",
        "Can introduce bias if data is not MCAR",
        "May create artificial peaks in distributions"
    ],
    "KNN Imputation": [
        "Computationally expensive for large datasets",
        "Sensitive to the choice of k",
        "May not work well with mixed data types",
        "Can be affected by the curse of dimensionality"
    ],
    "Forward/Backward Fill": [
        "Only suitable for time series data",
        "May introduce temporal bias",
        "Not appropriate for cross-sectional data"
    ]
}

for method, limits in limitations.items():
    print(f"\n{method}:")
    for limit in limits:
        print(f"  ‚Ä¢ {limit}")

# =============================================================================
# 4. HANDLING IMBALANCED DATASETS
# =============================================================================

print("\n‚öñÔ∏è SECTION 4: HANDLING IMBALANCED DATASETS")
print("-" * 45)

# Create an imbalanced dataset
X_imb, y_imb = make_classification(n_samples=1000, n_features=20, n_informative=10,
                                   n_redundant=5, n_clusters_per_class=1,
                                   weights=[0.9, 0.1], random_state=42)

# Convert to DataFrame for easier handling
feature_names = [f'feature_{i}' for i in range(X_imb.shape[1])]
df_imbalanced = pd.DataFrame(X_imb, columns=feature_names)
df_imbalanced['target'] = y_imb

print("üìä 4.1 Original Dataset Distribution:")
print(f"Class distribution:")
print(df_imbalanced['target'].value_counts())
print(f"Class distribution (%):")
print(df_imbalanced['target'].value_counts(normalize=True) * 100)

# Visualize class distribution
plt.figure(figsize=(12, 4))
plt.subplot(1, 3, 1)
df_imbalanced['target'].value_counts().plot(kind='bar')
plt.title('Original Class Distribution')
plt.xlabel('Class')
plt.ylabel('Count')

print("\nüîß 4.2 Techniques to Handle Imbalance:")

# Method 1: Random Undersampling
X_train, X_test, y_train, y_test = train_test_split(X_imb, y_imb, test_size=0.2, random_state=42)

undersampler = RandomUnderSampler(random_state=42)
X_under, y_under = undersampler.fit_resample(X_train, y_train)

print(f"\nAfter Random Undersampling:")
print(f"Original training size: {len(y_train)}")
print(f"Undersampled size: {len(y_under)}")
print(f"Class distribution: {np.bincount(y_under)}")

# Method 2: SMOTE (Synthetic Minority Over-sampling Technique)
smote = SMOTE(random_state=42)
X_smote, y_smote = smote.fit_resample(X_train, y_train)

print(f"\nAfter SMOTE:")
print(f"Original training size: {len(y_train)}")
print(f"SMOTE size: {len(y_smote)}")
print(f"Class distribution: {np.bincount(y_smote)}")

# Method 3: Class Weight Balancing
print(f"\nUsing Class Weights in Model:")

# Train models with different approaches
models = {
    'Original': RandomForestClassifier(random_state=42),
    'Undersampled': RandomForestClassifier(random_state=42),
    'SMOTE': RandomForestClassifier(random_state=42),
    'Class Weight': RandomForestClassifier(class_weight='balanced', random_state=42)
}

datasets = {
    'Original': (X_train, y_train),
    'Undersampled': (X_under, y_under),
    'SMOTE': (X_smote, y_smote),
    'Class Weight': (X_train, y_train)
}

# Plot class distributions
plt.subplot(1, 3, 2)
pd.Series(y_under).value_counts().plot(kind='bar')
plt.title('After Undersampling')
plt.xlabel('Class')
plt.ylabel('Count')

plt.subplot(1, 3, 3)
pd.Series(y_smote).value_counts().plot(kind='bar')
plt.title('After SMOTE')
plt.xlabel('Class')
plt.ylabel('Count')

plt.tight_layout()
plt.show()

# Evaluate all methods
print("\nüìà 4.3 Performance Comparison:")
for name, model in models.items():
    X_tr, y_tr = datasets[name]
    model.fit(X_tr, y_tr)
    predictions = model.predict(X_test)
    
    print(f"\n{name} Results:")
    print(classification_report(y_test, predictions, target_names=['Majority', 'Minority']))

# =============================================================================
# 5. SKEWNESS IN DATA AND HANDLING SKEWNESS
# =============================================================================

print("\nüìê SECTION 5: SKEWNESS IN DATA AND HANDLING SKEWNESS")
print("-" * 50)

# Create skewed data
np.random.seed(42)
normal_data = np.random.normal(50, 15, 1000)
right_skewed = np.random.exponential(2, 1000)
left_skewed = 10 - np.random.exponential(2, 1000)

skew_df = pd.DataFrame({
    'normal': normal_data,
    'right_skewed': right_skewed,
    'left_skewed': left_skewed
})

print("üìä 5.1 Understanding Skewness:")
from scipy import stats

for col in skew_df.columns:
    skewness = stats.skew(skew_df[col])
    print(f"{col}: Skewness = {skewness:.3f}")
    if abs(skewness) < 0.5:
        print(f"  ‚Üí Approximately symmetric")
    elif skewness > 0.5:
        print(f"  ‚Üí Right-skewed (positive skew)")
    else:
        print(f"  ‚Üí Left-skewed (negative skew)")

# Visualize skewness
plt.figure(figsize=(15, 5))
for i, col in enumerate(skew_df.columns):
    plt.subplot(1, 3, i+1)
    plt.hist(skew_df[col], bins=30, alpha=0.7, edgecolor='black')
    plt.title(f'{col.replace("_", " ").title()}\nSkewness: {stats.skew(skew_df[col]):.3f}')
    plt.xlabel('Value')
    plt.ylabel('Frequency')

plt.tight_layout()
plt.show()

print("\nüîß 5.2 Methods to Handle Skewness:")

# Focus on right-skewed data for transformation examples
right_skewed_positive = np.abs(right_skewed) + 1  # Ensure all values are positive

transformations = {
    'Original': right_skewed_positive,
    'Log Transform': np.log(right_skewed_positive),
    'Square Root': np.sqrt(right_skewed_positive),
    'Box-Cox': None  # Will be calculated below
}

# Box-Cox transformation
from scipy.stats import boxcox
boxcox_data, lambda_param = boxcox(right_skewed_positive)
transformations['Box-Cox'] = boxcox_data

print("Skewness after different transformations:")
for name, data in transformations.items():
    if data is not None:
        skewness = stats.skew(data)
        print(f"{name}: {skewness:.3f}")

# Visualize transformations
plt.figure(figsize=(20, 8))
for i, (name, data) in enumerate(transformations.items()):
    if data is not None:
        plt.subplot(2, 4, i+1)
        plt.hist(data, bins=30, alpha=0.7, edgecolor='black')
        plt.title(f'{name}\nSkewness: {stats.skew(data):.3f}')
        plt.xlabel('Value')
        plt.ylabel('Frequency')

# Q-Q plots to check normality
for i, (name, data) in enumerate(transformations.items()):
    if data is not None:
        plt.subplot(2, 4, i+5)
        stats.probplot(data, dist="norm", plot=plt)
        plt.title(f'{name} Q-Q Plot')

plt.tight_layout()
plt.show()

print("\nüéØ 5.3 Using PowerTransformer from sklearn:")

# PowerTransformer automatically finds the best transformation
pt_yeo_johnson = PowerTransformer(method='yeo-johnson', standardize=True)
pt_box_cox = PowerTransformer(method='box-cox', standardize=True)

# Yeo-Johnson can handle negative values, Box-Cox cannot
data_yj = pt_yeo_johnson.fit_transform(skew_df[['right_skewed']])
data_bc = pt_box_cox.fit_transform(skew_df[['right_skewed']].abs() + 1)

print("PowerTransformer Results:")
print(f"Original skewness: {stats.skew(skew_df['right_skewed']):.3f}")
print(f"Yeo-Johnson skewness: {stats.skew(data_yj.flatten()):.3f}")
print(f"Box-Cox skewness: {stats.skew(data_bc.flatten()):.3f}")

plt.figure(figsize=(15, 5))
plt.subplot(1, 3, 1)
plt.hist(skew_df['right_skewed'], bins=30, alpha=0.7)
plt.title('Original Data')

plt.subplot(1, 3, 2)
plt.hist(data_yj.flatten(), bins=30, alpha=0.7)
plt.title('Yeo-Johnson Transform')

plt.subplot(1, 3, 3)
plt.hist(data_bc.flatten(), bins=30, alpha=0.7)
plt.title('Box-Cox Transform')

plt.tight_layout()
plt.show()

print("\nüìã 5.4 When to Use Different Transformations:")
transformation_guide = {
    "Log Transformation": [
        "Best for right-skewed data with positive values",
        "Effective for data spanning several orders of magnitude",
        "Common in financial and biological data"
    ],
    "Square Root": [
        "Good for moderately right-skewed data",
        "Works with zero values (unlike log)",
        "Less aggressive than log transformation"
    ],
    "Box-Cox": [
        "Automatically finds optimal transformation parameter",
        "Only works with positive values",
        "More flexible than fixed transformations"
    ],
    "Yeo-Johnson": [
        "Works with positive, negative, and zero values",
        "More flexible than Box-Cox",
        "Good general-purpose transformation"
    ]
}

for method, guidelines in transformation_guide.items():
    print(f"\n{method}:")
    for guideline in guidelines:
        print(f"  ‚Ä¢ {guideline}")

# =============================================================================
# SUMMARY AND BEST PRACTICES
# =============================================================================

print("\nüéì SUMMARY AND BEST PRACTICES")
print("=" * 40)

best_practices = {
    "Missing Values": [
        "Always analyze the pattern of missingness (MCAR, MAR, MNAR)",
        "Consider domain knowledge when choosing imputation strategy",
        "Document your missing value handling decisions",
        "Consider multiple imputation for important analyses"
    ],
    "Imbalanced Data": [
        "Try simple methods first (class weights, stratified sampling)",
        "Use appropriate evaluation metrics (precision, recall, F1, AUC)",
        "Consider the cost of different types of errors",
        "Combine multiple techniques when necessary"
    ],
    "Skewness": [
        "Visualize data distributions before and after transformation",
        "Consider the interpretability of transformed features",
        "Not all algorithms require normally distributed features",
        "Document transformations for reproducibility"
    ]
}

for category, practices in best_practices.items():
    print(f"\n{category}:")
    for practice in practices:
        print(f"  ‚úì {practice}")

print("\nüéâ Tutorial Complete!")
print("Remember: Data preprocessing is often 80% of the machine learning workflow.")
print("Understanding these concepts deeply will make you a better ML engineer!")