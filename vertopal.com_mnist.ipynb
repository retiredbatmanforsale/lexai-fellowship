{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Machine Learning Case Study\n",
    "\n",
    "## A Comprehensive Guide to Machine Learning Fundamentals\n",
    "\n",
    "This notebook provides a detailed exploration of essential machine\n",
    "learning concepts using the classic MNIST handwritten digit dataset.\n",
    "We’ll combine theoretical explanations with practical Python\n",
    "implementations to build strong intuition and practical skills.\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "1.  [Introduction to the MNIST Dataset](#1-introduction)\n",
    "2.  [Data Exploration and Visualization](#2-exploration)\n",
    "3.  [Supervised Learning Fundamentals](#3-supervised-learning)\n",
    "4.  [Feature Engineering for Image Data](#4-feature-engineering)\n",
    "5.  [Linear Algebra and Probability\n",
    "    Foundations](#5-linear-algebra-probability)\n",
    "6.  [Understanding Covariance and Correlation](#6-covariance)\n",
    "7.  [Principal Component Analysis (PCA)](#7-pca)\n",
    "8.  [Data Distribution Analysis](#8-distributions)\n",
    "9.  [Cross-Validation Techniques](#9-cross-validation)\n",
    "10. [Bias-Variance Tradeoff](#10-bias-variance)\n",
    "11. [Overfitting and Underfitting](#11-overfitting-underfitting)\n",
    "12. [Regularization Methods](#12-regularization)\n",
    "13. [Classical Machine Learning Models](#13-classical-models)\n",
    "14. [Model Comparison and Analysis](#14-comparison)\n",
    "15. [Conclusion and Best Practices](#15-conclusion)\n",
    "\n",
    "## 1. Introduction to the MNIST Dataset <a id=\"1-introduction\"></a>\n",
    "\n",
    "The MNIST dataset is one of the most widely used benchmark datasets in\n",
    "machine learning. It consists of 28×28 grayscale images of handwritten\n",
    "digits (0-9).\n",
    "\n",
    "Let’s begin by loading the dataset and understanding its structure.\n",
    "\n",
    "``` python\n",
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import metrics, preprocessing, model_selection\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting aesthetics\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "```\n",
    "\n",
    "``` python\n",
    "# Load the MNIST dataset\n",
    "train_data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')\n",
    "\n",
    "print(f\"Training data shape: {train_data.shape}\")\n",
    "print(f\"Testing data shape: {test_data.shape}\")\n",
    "\n",
    "# Display the first few rows\n",
    "train_data.head()\n",
    "```\n",
    "\n",
    "### Dataset Structure\n",
    "\n",
    "-   Each image is 28×28 pixels, flattened into 784 features\n",
    "-   Pixel values range from 0 (white) to 255 (black)\n",
    "-   The training set includes 60,000 examples\n",
    "-   The test set includes 10,000 examples\n",
    "-   The first column in the training set contains the label (0-9)\n",
    "\n",
    "## 2. Data Exploration and Visualization <a id=\"2-exploration\"></a>\n",
    "\n",
    "Let’s explore and visualize the dataset to better understand the data\n",
    "we’re working with.\n",
    "\n",
    "``` python\n",
    "# Basic statistics of the training data\n",
    "train_data.describe()\n",
    "```\n",
    "\n",
    "``` python\n",
    "# Class distribution - how many examples of each digit?\n",
    "plt.figure(figsize=(10, 6))\n",
    "class_distribution = train_data['label'].value_counts().sort_index()\n",
    "ax = sns.barplot(x=class_distribution.index, y=class_distribution.values)\n",
    "plt.title('Distribution of Digits in the Training Set', fontsize=14)\n",
    "plt.xlabel('Digit', fontsize=12)\n",
    "plt.ylabel('Count', fontsize=12)\n",
    "for i, v in enumerate(class_distribution.values):\n",
    "    ax.text(i, v + 100, str(v), ha='center')\n",
    "plt.show()\n",
    "\n",
    "print(\"Class distribution:\")\n",
    "for digit, count in class_distribution.items():\n",
    "    print(f\"Digit {digit}: {count} samples ({count/len(train_data)*100:.2f}%)\")\n",
    "```\n",
    "\n",
    "Now, let’s visualize some examples of the digits:\n",
    "\n",
    "``` python\n",
    "# Function to plot a digit\n",
    "def plot_digit(data, index):\n",
    "    digit = data.iloc[index, 1:].values.reshape(28, 28)\n",
    "    plt.imshow(digit, cmap='gray_r')\n",
    "    plt.title(f\"Label: {data.iloc[index, 0]}\")\n",
    "    plt.axis('off')\n",
    "\n",
    "# Function to plot multiple digits\n",
    "def plot_multiple_digits(data, nrows=5, ncols=5):\n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=(12, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # Get random indices\n",
    "    random_indices = np.random.randint(0, len(data), nrows * ncols)\n",
    "    \n",
    "    for i, ax in enumerate(axes):\n",
    "        idx = random_indices[i]\n",
    "        digit = data.iloc[idx, 1:].values.reshape(28, 28)\n",
    "        ax.imshow(digit, cmap='gray_r')\n",
    "        ax.set_title(f\"Label: {data.iloc[idx, 0]}\")\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot random sample of digits\n",
    "plot_multiple_digits(train_data)\n",
    "```\n",
    "\n",
    "``` python\n",
    "# Let's analyze the pixel intensity distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "# Get all pixel values and create a histogram\n",
    "pixel_values = train_data.iloc[:, 1:].values.flatten()\n",
    "plt.hist(pixel_values, bins=50, color='blue', alpha=0.7)\n",
    "plt.title('Distribution of Pixel Intensities Across All Images', fontsize=14)\n",
    "plt.xlabel('Pixel Value (0-255)', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Mean pixel value: {pixel_values.mean():.2f}\")\n",
    "print(f\"Standard deviation: {pixel_values.std():.2f}\")\n",
    "print(f\"Min pixel value: {pixel_values.min()}\")\n",
    "print(f\"Max pixel value: {pixel_values.max()}\")\n",
    "print(f\"Percentage of pixels with zero value: {(pixel_values == 0).mean() * 100:.2f}%\")\n",
    "```\n",
    "\n",
    "``` python\n",
    "# Visualize average digit for each class\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "for i in range(10):\n",
    "    # Select all examples with the current digit\n",
    "    digit_data = train_data[train_data['label'] == i].iloc[:, 1:].values\n",
    "    \n",
    "    # Calculate the average pixel values\n",
    "    avg_digit = digit_data.mean(axis=0).reshape(28, 28)\n",
    "    \n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    plt.imshow(avg_digit, cmap='gray_r')\n",
    "    plt.title(f\"Average of Digit: {i}\")\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "## 3. Supervised Learning Fundamentals <a id=\"3-supervised-learning\"></a>\n",
    "\n",
    "MNIST classification is a quintessential supervised learning problem: -\n",
    "We have labeled examples (digits 0-9) - We want to learn a mapping from\n",
    "pixel features to digit labels - We will train on known examples and\n",
    "predict on unknown examples\n",
    "\n",
    "### Key Elements of Supervised Learning\n",
    "\n",
    "``` python\n",
    "# Basic approach to supervised learning with MNIST\n",
    "\n",
    "# 1. Separate features (X) and labels (y)\n",
    "X_train = train_data.iloc[:, 1:].values\n",
    "y_train = train_data.iloc[:, 0].values\n",
    "\n",
    "# 2. Create a validation set\n",
    "# We'll use 20% of the training data as a validation set\n",
    "X_train, X_val, y_train, y_val = model_selection.train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]} examples\")\n",
    "print(f\"Validation set size: {X_val.shape[0]} examples\")\n",
    "\n",
    "# 3. Normalize the features (scale pixel values to [0, 1])\n",
    "X_train = X_train / 255.0\n",
    "X_val = X_val / 255.0\n",
    "\n",
    "# 4. Define evaluation metrics for classification\n",
    "def evaluate_model(model, X, y_true, name=\"Model\"):\n",
    "    \"\"\"Evaluate model performance with multiple metrics.\"\"\"\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = metrics.accuracy_score(y_true, y_pred)\n",
    "    f1 = metrics.f1_score(y_true, y_pred, average='weighted')\n",
    "    precision = metrics.precision_score(y_true, y_pred, average='weighted')\n",
    "    recall = metrics.recall_score(y_true, y_pred, average='weighted')\n",
    "    \n",
    "    print(f\"{name} Performance:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"F1 Score (weighted): {f1:.4f}\")\n",
    "    print(f\"Precision (weighted): {precision:.4f}\")\n",
    "    print(f\"Recall (weighted): {recall:.4f}\")\n",
    "    \n",
    "    # Confusion matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    cm = metrics.confusion_matrix(y_true, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(f'Confusion Matrix - {name}', fontsize=14)\n",
    "    plt.xlabel('Predicted Label', fontsize=12)\n",
    "    plt.ylabel('True Label', fontsize=12)\n",
    "    plt.show()\n",
    "    \n",
    "    return accuracy, f1, precision, recall\n",
    "```\n",
    "\n",
    "### Theory of Supervised Learning\n",
    "\n",
    "Supervised learning is about finding a function (f) that maps inputs (X)\n",
    "to outputs (y): \\[ y = f(X) + \\]\n",
    "\n",
    "This function is learned from labeled examples by minimizing a loss\n",
    "function that measures the difference between predictions and true\n",
    "labels.\n",
    "\n",
    "For classification problems like MNIST, we use: - **Loss functions**:\n",
    "Cross-entropy, hinge loss - **Evaluation metrics**: Accuracy, precision,\n",
    "recall, F1-score - **Learning algorithms**: Decision trees, neural\n",
    "networks, SVMs, etc.\n",
    "\n",
    "## 4. Feature Engineering for Image Data <a id=\"4-feature-engineering\"></a>\n",
    "\n",
    "While deep learning often works with raw pixels, traditional machine\n",
    "learning benefits significantly from feature engineering. Let’s explore\n",
    "some techniques:\n",
    "\n",
    "``` python\n",
    "# Original features: raw pixel values\n",
    "print(f\"Original feature space dimension: {X_train.shape[1]}\")\n",
    "\n",
    "# Some basic feature engineering approaches:\n",
    "\n",
    "# 1. HISTOGRAM OF ORIENTED GRADIENTS (HOG)\n",
    "from skimage.feature import hog\n",
    "from skimage import exposure\n",
    "\n",
    "def get_hog_features(X, visualize=False):\n",
    "    # Take a small sample to demonstrate\n",
    "    sample_digit = X[0].reshape(28, 28)\n",
    "    \n",
    "    # Extract HOG features\n",
    "    hog_features, hog_image = hog(\n",
    "        sample_digit, \n",
    "        orientations=9, \n",
    "        pixels_per_cell=(4, 4),\n",
    "        cells_per_block=(2, 2), \n",
    "        visualize=True,\n",
    "        block_norm='L2-Hys'\n",
    "    )\n",
    "    \n",
    "    if visualize:\n",
    "        # Rescale HOG image for better visualization\n",
    "        hog_image_rescaled = exposure.rescale_intensity(hog_image, in_range=(0, 10))\n",
    "        \n",
    "        # Plot\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "        ax1.imshow(sample_digit, cmap='gray_r')\n",
    "        ax1.set_title('Original Image')\n",
    "        ax1.axis('off')\n",
    "        \n",
    "        ax2.imshow(hog_image_rescaled, cmap='gray_r')\n",
    "        ax2.set_title('HOG Features')\n",
    "        ax2.axis('off')\n",
    "        plt.show()\n",
    "    \n",
    "    return hog_features\n",
    "\n",
    "# Demonstrate HOG features\n",
    "hog_features = get_hog_features(X_train, visualize=True)\n",
    "print(f\"HOG feature space dimension: {len(hog_features)}\")\n",
    "```\n",
    "\n",
    "``` python\n",
    "# 2. ZONING - Divide image into zones and calculate statistics\n",
    "def get_zoning_features(X, n_zones=4, visualize=False):\n",
    "    # Reshape for easier zoning\n",
    "    sample_digit = X[0].reshape(28, 28)\n",
    "    \n",
    "    # Define zone size\n",
    "    zone_height = 28 // n_zones\n",
    "    zone_width = 28 // n_zones\n",
    "    \n",
    "    # Create features based on zone statistics\n",
    "    zoning_features = []\n",
    "    \n",
    "    if visualize:\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.imshow(sample_digit, cmap='gray_r')\n",
    "        \n",
    "    for i in range(n_zones):\n",
    "        for j in range(n_zones):\n",
    "            # Extract zone\n",
    "            zone = sample_digit[i*zone_height:(i+1)*zone_height, \n",
    "                               j*zone_width:(j+1)*zone_width]\n",
    "            \n",
    "            # Calculate statistics\n",
    "            zone_mean = zone.mean()\n",
    "            zone_std = zone.std()\n",
    "            zone_max = zone.max()\n",
    "            \n",
    "            # Add features\n",
    "            zoning_features.extend([zone_mean, zone_std, zone_max])\n",
    "            \n",
    "            if visualize:\n",
    "                # Draw zone boundaries\n",
    "                plt.plot([j*zone_width, j*zone_width], \n",
    "                         [i*zone_height, (i+1)*zone_height], 'r-')\n",
    "                plt.plot([(j+1)*zone_width, (j+1)*zone_width], \n",
    "                         [i*zone_height, (i+1)*zone_height], 'r-')\n",
    "                plt.plot([j*zone_width, (j+1)*zone_width], \n",
    "                         [i*zone_height, i*zone_height], 'r-')\n",
    "                plt.plot([j*zone_width, (j+1)*zone_width], \n",
    "                         [(i+1)*zone_height, (i+1)*zone_height], 'r-')\n",
    "                \n",
    "                # Add text for zone statistics\n",
    "                plt.text(j*zone_width + zone_width/2, i*zone_height + zone_height/2, \n",
    "                         f\"{zone_mean:.1f}\", color='blue',\n",
    "                         horizontalalignment='center', verticalalignment='center')\n",
    "    \n",
    "    if visualize:\n",
    "        plt.title('Zoning with Mean Values', fontsize=14)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "    \n",
    "    return np.array(zoning_features)\n",
    "\n",
    "# Demonstrate zoning features\n",
    "zoning_features = get_zoning_features(X_train, visualize=True)\n",
    "print(f\"Zoning feature space dimension: {len(zoning_features)}\")\n",
    "```\n",
    "\n",
    "``` python\n",
    "# 3. HISTOGRAM FEATURES\n",
    "def get_histogram_features(X, n_bins=10, visualize=False):\n",
    "    # Take sample digit\n",
    "    sample_digit = X[0].reshape(28, 28)\n",
    "    \n",
    "    # Calculate histogram\n",
    "    hist, bin_edges = np.histogram(sample_digit, bins=n_bins, range=(0, 1))\n",
    "    hist = hist / np.sum(hist)  # Normalize\n",
    "    \n",
    "    if visualize:\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        \n",
    "        # Plot original image\n",
    "        ax1.imshow(sample_digit, cmap='gray_r')\n",
    "        ax1.set_title('Original Image')\n",
    "        ax1.axis('off')\n",
    "        \n",
    "        # Plot histogram\n",
    "        ax2.bar(range(n_bins), hist, width=0.8, \n",
    "                align='center', color='blue', alpha=0.7)\n",
    "        ax2.set_xlabel('Pixel Intensity Bin')\n",
    "        ax2.set_ylabel('Normalized Frequency')\n",
    "        ax2.set_title('Histogram Features')\n",
    "        ax2.set_xticks(range(n_bins))\n",
    "        ax2.set_xticklabels([f\"{edge:.1f}\" for edge in bin_edges[:-1]])\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    return hist\n",
    "\n",
    "# Demonstrate histogram features\n",
    "hist_features = get_histogram_features(X_train, visualize=True)\n",
    "print(f\"Histogram feature space dimension: {len(hist_features)}\")\n",
    "```\n",
    "\n",
    "### Applying Feature Engineering to the Dataset\n",
    "\n",
    "Let’s implement a simple feature extraction pipeline to demonstrate how\n",
    "we could prepare data for traditional models:\n",
    "\n",
    "``` python\n",
    "# Function to extract features from multiple images\n",
    "def extract_features(X, n_samples=None, n_zones=4, n_bins=10):\n",
    "    \"\"\"Extract multiple types of features from images.\"\"\"\n",
    "    if n_samples is None:\n",
    "        n_samples = len(X)\n",
    "    else:\n",
    "        n_samples = min(n_samples, len(X))\n",
    "    \n",
    "    # Sample selection\n",
    "    indices = np.random.choice(len(X), n_samples, replace=False)\n",
    "    X_sample = X[indices]\n",
    "    \n",
    "    # Initialize feature array\n",
    "    all_features = []\n",
    "    \n",
    "    for img in X_sample:\n",
    "        img_reshaped = img.reshape(28, 28)\n",
    "        \n",
    "        # Get zoning features\n",
    "        zone_features = []\n",
    "        zone_height = 28 // n_zones\n",
    "        zone_width = 28 // n_zones\n",
    "        \n",
    "        for i in range(n_zones):\n",
    "            for j in range(n_zones):\n",
    "                zone = img_reshaped[i*zone_height:(i+1)*zone_height, \n",
    "                                   j*zone_width:(j+1)*zone_width]\n",
    "                zone_mean = zone.mean()\n",
    "                zone_std = zone.std()\n",
    "                zone_max = zone.max()\n",
    "                zone_features.extend([zone_mean, zone_std, zone_max])\n",
    "        \n",
    "        # Get histogram features\n",
    "        hist, _ = np.histogram(img, bins=n_bins, range=(0, 1))\n",
    "        hist = hist / np.sum(hist)  # Normalize\n",
    "        \n",
    "        # Get simple edge features (difference between adjacent pixels)\n",
    "        edges_h = np.abs(np.diff(img_reshaped, axis=0))\n",
    "        edges_v = np.abs(np.diff(img_reshaped, axis=1))\n",
    "        edge_features = [edges_h.mean(), edges_h.std(), edges_v.mean(), edges_v.std()]\n",
    "        \n",
    "        # Combine all features\n",
    "        features = np.concatenate([zone_features, hist, edge_features])\n",
    "        all_features.append(features)\n",
    "    \n",
    "    return np.array(all_features)\n",
    "\n",
    "# Extract features from a small sample for demonstration\n",
    "X_train_features = extract_features(X_train, n_samples=5000)\n",
    "X_val_features = extract_features(X_val, n_samples=1000)\n",
    "\n",
    "print(f\"Engineered feature space dimension: {X_train_features.shape[1]}\")\n",
    "```\n",
    "\n",
    "## 5. Linear Algebra and Probability Foundations <a id=\"5-linear-algebra-probability\"></a>\n",
    "\n",
    "Machine learning algorithms leverage linear algebra and probability\n",
    "theory extensively. Let’s explore key concepts using our MNIST data.\n",
    "\n",
    "### Linear Algebra Concepts with MNIST\n",
    "\n",
    "``` python\n",
    "# 1. Matrix operations with images\n",
    "\n",
    "# Select single digit\n",
    "digit = X_train[0].reshape(28, 28)\n",
    "\n",
    "# Matrix operations\n",
    "# Transpose - Flip along diagonal\n",
    "digit_transposed = digit.T\n",
    "\n",
    "# Add noise to create a related but different image\n",
    "noise = np.random.normal(0, 0.1, size=digit.shape)\n",
    "digit_noisy = np.clip(digit + noise, 0, 1)\n",
    "\n",
    "# Matrix multiplication (convolution-like operation)\n",
    "# Create a simple edge detection kernel\n",
    "edge_kernel = np.array([\n",
    "    [-1, -1, -1],\n",
    "    [-1,  8, -1],\n",
    "    [-1, -1, -1]\n",
    "])\n",
    "\n",
    "# Apply kernel via correlation (similar to convolution)\n",
    "from scipy import signal\n",
    "digit_edges = signal.correlate2d(digit, edge_kernel, mode='same')\n",
    "\n",
    "# Visualize\n",
    "fig, axs = plt.subplots(1, 4, figsize=(16, 4))\n",
    "axs[0].imshow(digit, cmap='gray_r')\n",
    "axs[0].set_title('Original')\n",
    "axs[0].axis('off')\n",
    "\n",
    "axs[1].imshow(digit_transposed, cmap='gray_r')\n",
    "axs[1].set_title('Transposed')\n",
    "axs[1].axis('off')\n",
    "\n",
    "axs[2].imshow(digit_noisy, cmap='gray_r')\n",
    "axs[2].set_title('With Noise')\n",
    "axs[2].axis('off')\n",
    "\n",
    "axs[3].imshow(digit_edges, cmap='gray_r')\n",
    "axs[3].set_title('Edge Detection')\n",
    "axs[3].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "``` python\n",
    "# 2. Vector spaces and distances\n",
    "from sklearn.metrics.pairwise import euclidean_distances, cosine_similarity\n",
    "\n",
    "# Select two digits for comparison\n",
    "digit1 = X_train[0]  # First image\n",
    "digit2 = X_train[1]  # Second image\n",
    "digit3 = X_train[np.where(y_train == y_train[0])[0][1]]  # Another image of same class as digit1\n",
    "\n",
    "# Calculate distances\n",
    "euclidean_1_2 = euclidean_distances([digit1], [digit2])[0][0]\n",
    "euclidean_1_3 = euclidean_distances([digit1], [digit3])[0][0]\n",
    "\n",
    "cosine_1_2 = cosine_similarity([digit1], [digit2])[0][0]\n",
    "cosine_1_3 = cosine_similarity([digit1], [digit3])[0][0]\n",
    "\n",
    "# Visualize\n",
    "fig, axs = plt.subplots(1, 3, figsize=(12, 4))\n",
    "axs[0].imshow(digit1.reshape(28, 28), cmap='gray_r')\n",
    "axs[0].set_title(f'Digit 1 (Class: {y_train[0]})')\n",
    "axs[0].axis('off')\n",
    "\n",
    "axs[1].imshow(digit2.reshape(28, 28), cmap='gray_r')\n",
    "axs[1].set_title(f'Digit 2 (Class: {y_train[1]})')\n",
    "axs[1].axis('off')\n",
    "\n",
    "axs[2].imshow(digit3.reshape(28, 28), cmap='gray_r')\n",
    "axs[2].set_title(f'Digit 3 (Class: {y_train[np.where(y_train == y_train[0])[0][1]]})')\n",
    "axs[2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Distance metrics:\")\n",
    "print(f\"Euclidean distance between Digit 1 and Digit 2 (different classes): {euclidean_1_2:.4f}\")\n",
    "print(f\"Euclidean distance between Digit 1 and Digit 3 (same class): {euclidean_1_3:.4f}\")\n",
    "print()\n",
    "print(f\"Cosine similarity between Digit 1 and Digit 2 (different classes): {cosine_1_2:.4f}\")\n",
    "print(f\"Cosine similarity between Digit 1 and Digit 3 (same class): {cosine_1_3:.4f}\")\n",
    "```\n",
    "\n",
    "### Probability Concepts with MNIST\n",
    "\n",
    "``` python\n",
    "# 1. Class probabilities\n",
    "class_probs = train_data['label'].value_counts(normalize=True).sort_index()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = sns.barplot(x=class_probs.index, y=class_probs.values)\n",
    "plt.title('Probability Distribution of Digits', fontsize=14)\n",
    "plt.xlabel('Digit', fontsize=12)\n",
    "plt.ylabel('Probability', fontsize=12)\n",
    "for i, v in enumerate(class_probs):\n",
    "    ax.text(i, v + 0.005, f\"{v:.3f}\", ha='center')\n",
    "plt.show()\n",
    "\n",
    "# Prior probabilities\n",
    "print(\"Prior class probabilities:\")\n",
    "for digit, prob in class_probs.items():\n",
    "    print(f\"P(Digit = {digit}) = {prob:.4f}\")\n",
    "```\n",
    "\n",
    "``` python\n",
    "# 2. Pixel intensity distributions by class\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "for i in range(10):\n",
    "    # Get all samples of this digit\n",
    "    digit_samples = train_data[train_data['label'] == i].iloc[:, 1:].values\n",
    "    \n",
    "    # Get average pixel values\n",
    "    avg_intensity = digit_samples.mean(axis=0)\n",
    "    \n",
    "    # Plot histogram of average intensities\n",
    "    plt.subplot(3, 4, i+1)\n",
    "    plt.hist(avg_intensity, bins=50, alpha=0.7)\n",
    "    plt.title(f'Pixel Intensity Distribution: Digit {i}')\n",
    "    plt.xlabel('Average Pixel Value')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.xlim(0, 255)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "``` python\n",
    "# 3. Bayes' Theorem in action - Probabilistic classification\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Train a Naive Bayes classifier\n",
    "nb_classifier = GaussianNB()\n",
    "nb_classifier.fit(X_train[:5000], y_train[:5000])  # Use a small subset for speed\n",
    "\n",
    "# Get probabilities for a single example\n",
    "test_digit = X_val[0]\n",
    "test_digit_class = y_val[0]\n",
    "class_probabilities = nb_classifier.predict_proba([test_digit])[0]\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(test_digit.reshape(28, 28), cmap='gray_r')\n",
    "plt.title(f'Test Digit (True Class: {test_digit_class})', fontsize=14)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "bars = plt.bar(range(10), class_probabilities)\n",
    "plt.title('Class Probabilities (Naive Bayes)', fontsize=14)\n",
    "plt.xlabel('Digit Class', fontsize=12)\n",
    "plt.ylabel('Probability', fontsize=12)\n",
    "plt.xticks(range(10))\n",
    "\n",
    "# Highlight the true class\n",
    "bars[test_digit_class].set_color('red')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Predicted class: {nb_classifier.predict([test_digit])[0]}\")\n",
    "print(f\"True class: {test_digit_class}\")\n",
    "print(\"\\nClass probabilities:\")\n",
    "for i, prob in enumerate(class_probabilities):\n",
    "    print(f\"P(Digit = {i} | Image) = {prob:.4f}\")\n",
    "```\n",
    "\n",
    "## 6. Understanding Covariance and Correlation <a id=\"6-covariance\"></a>\n",
    "\n",
    "Covariance measures how two variables change together, revealing\n",
    "relationships between features. Let’s explore covariance in MNIST data:\n",
    "\n",
    "``` python\n",
    "# Select a small subset of pixels to analyze covariance\n",
    "# Choose pixels from the middle of the image (more interesting)\n",
    "central_pixels = [\n",
    "    # Row indices from middle of image\n",
    "    392, 393, 394, 395,  # Middle row pixels\n",
    "    364, 365, 366, 367   # Pixels from row above\n",
    "]\n",
    "\n",
    "# Create a dataframe with these pixels\n",
    "pixel_sample = train_data.iloc[:1000, [0] + [i+1 for i in central_pixels]]\n",
    "pixel_sample.columns = ['label'] + [f'pixel_{i}' for i in central_pixels]\n",
    "\n",
    "# Calculate covariance matrix\n",
    "cov_matrix = pixel_sample.iloc[:, 1:].cov()\n",
    "\n",
    "# Calculate correlation matrix\n",
    "corr_matrix = pixel_sample.iloc[:, 1:].corr()\n",
    "\n",
    "# Visualize covariance matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cov_matrix, annot=True, fmt='.2f', cmap='coolwarm')\n",
    "plt.title('Covariance Matrix of Central Pixels', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Visualize correlation matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.title('Correlation Matrix of Central Pixels', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "``` python\n",
    "# Show what these pixels look like on an image\n",
    "def highlight_pixels(pixel_indices, image_shape=(28, 28)):\n",
    "    \"\"\"Create a visualization of where selected pixels are located in the image.\"\"\"\n",
    "    # Create empty image\n",
    "    image = np.zeros(image_shape)\n",
    "    \n",
    "    # Fill in the selected pixels\n",
    "    for idx in pixel_indices:\n",
    "        # Convert flattened index back to 2D\n",
    "        row = idx // image_shape[1]\n",
    "        col = idx % image_shape[1]\n",
    "        image[row, col] = 1\n",
    "    \n",
    "    return image\n",
    "\n",
    "# Create visualization\n",
    "highlighted = highlight_pixels(central_pixels)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(highlighted, cmap='gray_r')\n",
    "plt.title('Location of Selected Pixels for Covariance Analysis', fontsize=14)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "``` python\n",
    "# Visualize covariance across different digits\n",
    "plt.figure(figsize=(15, 12))\n",
    "\n",
    "for i in range(10):\n",
    "    # Get samples for this digit\n",
    "    digit_pixels = train_data[train_data['label'] == i].iloc[:200, 1:].values\n",
    "    \n",
    "    # Calculate covariance matrix (use only subset of pixels for visualization)\n",
    "    # Take a 10x10 grid from the center\n",
    "    center_start_row = 9\n",
    "    center_start_col = 9\n",
    "    center_pixels = []\n",
    "    \n",
    "    for r in range(10):\n",
    "        for c in range(10):\n",
    "            idx = (center_start_row + r) * 28 + (center_start_col + c)\n",
    "            center_pixels.append(idx)\n",
    "    \n",
    "    # Extract these pixels\n",
    "    digit_center = digit_pixels[:, center_pixels]\n",
    "    \n",
    "    # Calculate covariance\n",
    "    cov = np.cov(digit_center, rowvar=False)\n",
    "    \n",
    "    # Plot\n",
    "    plt.subplot(2, 5, i+1)\n",
    "    sns.heatmap(cov, cmap='coolwarm', xticklabels=False, yticklabels=False)\n",
    "    plt.title(f'Covariance: Digit {i}', fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "``` python\n",
    "# Interpreting covariance in classification context\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "# Use small subset for demonstration\n",
    "X_sample = X_train[:5000]\n",
    "y_sample = y_train[:5000]\n",
    "\n",
    "# Fit LDA model\n",
    "lda = LinearDiscriminantAnalysis(n_components=2)\n",
    "X_lda = lda.fit_transform(X_sample, y_sample)\n",
    "\n",
    "# Plot classes in LDA space\n",
    "plt.figure(figsize=(12, 10))\n",
    "scatter = plt.scatter(X_lda[:, 0], X_lda[:, 1], c=y_sample, \n",
    "                      cmap='tab10', alpha=0.6, s=20)\n",
    "plt.colorbar(scatter, label='Digit')\n",
    "plt.title('LDA Projects Data Based on Covariance Structure', fontsize=14)\n",
    "plt.xlabel('LD 1', fontsize=12)\n",
    "plt.ylabel('LD 2', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"LDA leverages class covariance matrices to find the most discriminative projections.\")\n",
    "print(f\"LDA explained variance ratio: {lda.explained_variance_ratio_}\")\n",
    "```\n",
    "\n",
    "## 7. Principal Component Analysis (PCA) <a id=\"7-pca\"></a>\n",
    "\n",
    "PCA is a crucial dimensionality reduction technique that projects data\n",
    "onto directions of maximum variance.\n",
    "\n",
    "\\`\\`\\`python from sklearn.decomposition import PCA\n",
    "\n",
    "# Apply P"
   ],
   "id": "42a14256-91c8-446e-92a7-ab6bf11055d3"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
