{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d8c81f97-de2f-49d0-adb9-f897fa75d8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c8dbdffd-e726-4993-8bfe-ab8203b6a32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNeuralNetwork:\n",
    "    \"\"\"\n",
    "    A simple neural network with one hidden layer. We know that we need to define the architecture of our NN, then we need to implement a forwards pass, \n",
    "    then we need to have something known as a backward pass or backprop, then finally we can have a training function that relies on forward and backward \n",
    "    passes. That's it. \n",
    "    \n",
    "    Architecture: [number of Input(s) which depends on the size of input (feature vector), hidden layer (arbitrary number, hyperparameter), output layer]\n",
    "    - Input layer: variable size (depends on your input data) \n",
    "        if input is 10-dimensional feature vector, we will have 10 input neurons in the input layer. \n",
    "    - Hidden layer: configurable number of neurons (hyperparameters)\n",
    "        Neurons in hidden layer are not dependent on the input layer, they are a hyperparameter. We choose it based on the problem complexity,\n",
    "        model capacity & overfitting risk. The number of neurons is a design choice here. \n",
    "        The bias is not a neuron in the input layer. Biases are added in the next set of layers. Hidden and output. \n",
    "    - Output layer: 1 neuron (for binary classification)\n",
    "        Since it is a binary classification problem, we only have one neuron that will represent the probability of class 1. \n",
    "        if P(class = 1) = k, then P(class = 2) = 1 - k\n",
    "    \n",
    "    This network uses:\n",
    "    - Sigmoid activation function. For modelling the probabilities. \n",
    "    - Mean squared error loss. As our loss function\n",
    "    - Basic gradient descent optimization. To find the optimal values of our parameters. \n",
    "    \"\"\"\n",
    "    # size of layers and learning rate \n",
    "    '''\n",
    "    1st step: Init the arch. \n",
    "    '''\n",
    "    def __init__(self, input_size, hidden_size, learning_rate=0.01): \n",
    "        \"\"\"\n",
    "        Initialize the neural network.\n",
    "        \n",
    "        Args:\n",
    "            input_size (int): Number of input features\n",
    "            hidden_size (int): Number of neurons in hidden layer\n",
    "            learning_rate (float): How fast the network learns\n",
    "        \"\"\"\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.learning_rate = learning_rate\n",
    "        '''\n",
    "        2nd step: Initialise all weight(s) vector and bias(es). \n",
    "        # Initialize weights randomly (small values work better)\n",
    "        # Weights from input to hidden layer h1\n",
    "        # size of W1 = (input x hidden) \n",
    "        '''\n",
    "\n",
    "        # connections flowing from input --> h1\n",
    "        # size of W1 = (neurons in input = 10 , neruons in hidden = 2) 20 parameters so far\n",
    "        self.W1 = np.random.randn(self.input_size, self.hidden_size) * 0.5\n",
    "        '''\n",
    "        # Bias for hidden layer \n",
    "        # size of b1 = (1 x number of neurons in hidden layers)\n",
    "        ''' \n",
    "        # number of biases b1 == number of neurons in h1. # 20 + 2 =2 parameters so far\n",
    "        self.b1 = np.zeros((1, self.hidden_size))\n",
    "        \n",
    "        \n",
    "        \n",
    "        '''\n",
    "        # Weights from hidden to output layer\n",
    "        # size of W2 = (number of neurons in hidden layers x 1)\n",
    "        '''\n",
    "        # connections between hidden layer --> output # 22 + 2 = 24 parameters so far\n",
    "        # size of w2 = (2, 1) \n",
    "        self.W2 = np.random.randn(self.hidden_size, 1) * 0.5\n",
    "        '''\n",
    "        # Bias for output layer\n",
    "        # size of b2 = (1x1)\n",
    "        '''\n",
    "        # since output has only one neuron, thus 1x1. --> 25 parameters in total, so far.\n",
    "        self.b2 = np.zeros((1, 1))\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        \"\"\"\n",
    "        Sigmoid activation function.\n",
    "        Maps any real number to a value between 0 and 1.\n",
    "        Formula: 1 / (1 + e^(-x))\n",
    "        \n",
    "        # why Clip x ? --> to prevent overflow\n",
    "        # if x becomes a very large negative number, say -1000, then: np.exp() will give Inf values\n",
    "        \"\"\"\n",
    "        x = np.clip(x, -500, 500)\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "\n",
    "    '''\n",
    "    Why do we need a derivate ever, to find the optimal values of all our parameters. So assume that derivative of a sigmoid \n",
    "    is equal to sigmoid(x) * (1 - sigmoid(x))\n",
    "    '''\n",
    "    def sigmoid_derivative(self, x):\n",
    "        \"\"\"\n",
    "        Derivative of sigmoid function.\n",
    "        Used during backpropagation.\n",
    "        Formula: sigmoid(x) * (1 - sigmoid(x))\n",
    "        Args: \n",
    "            x is not a naked x, it is a sigmoid(x)\n",
    "        \"\"\"\n",
    "        return x * (1 - x)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward pass: compute predictions. Nothing but some stupid matrix multiplication\n",
    "        \n",
    "        Args:\n",
    "            X (numpy array): Input data, shape (n_samples, n_features)\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (hidden_output, final_output)\n",
    "        \"\"\"\n",
    "        '''\n",
    "        # Step 1: Input to Hidden Layer\n",
    "        # Multiply inputs by weights and add bias\n",
    "        # z1 = (X) W1 + b1. Here X is the input from input layer \n",
    "        '''\n",
    "        self.z1 = X@self.W1 + self.b1 # W1,b1\n",
    "        \n",
    "        # Apply activation function\n",
    "        self.a1 = self.sigmoid(self.z1) \n",
    "        \n",
    "        \n",
    "        '''\n",
    "        # Step 2: Hidden to Output Layer\n",
    "        # Multiply hidden outputs by weights and add bias\n",
    "        # z2 = (X) W2 + b. Here X is the input from the previous layer. \n",
    "        '''\n",
    "        self.z2 =  self.a1@self.W2 + self.b2\n",
    "        # Apply activation function to get final prediction\n",
    "        self.a2 = self.sigmoid(self.z2)    \n",
    "        \n",
    "        return self.a1, self.a2\n",
    "    \n",
    "    def backward(self, X, y, output): # backpropogation's algorithm. \n",
    "        \"\"\"\n",
    "        Backward pass: compute gradients and update weights. Nothing but some stupid calculus to find optimal values \n",
    "        of gradients/parameters. \n",
    "        This is where the network learns from its mistakes.\n",
    "\n",
    "        This algorithm simply helps me compute the value of gradients/parameters/co-efficients/weight & bias vectors.         \n",
    "        Args:\n",
    "            X (numpy array): Input data\n",
    "            y (numpy array): True labels\n",
    "            output (numpy array): Network predictions\n",
    "        \"\"\"\n",
    "        m = X.shape[0]  # Number of training examples\n",
    "        \n",
    "        # Step 1: Calculate output layer error\n",
    "        # How wrong were our predictions? output_error=true_label − predicted_probability\n",
    "        output_error = y - output\n",
    "        # How much should we change the output? (gradient)\n",
    "        # output_delta = output_error⋅σ'(z)\n",
    "        output_delta = output_error * self.sigmoid_derivative(output)\n",
    "        \n",
    "        # Step 2: Calculate hidden layer error\n",
    "        # Propagate the error backwards\n",
    "        hidden_error = output_delta.dot(self.W2.T)\n",
    "        # How much should we change the hidden layer?\n",
    "        hidden_delta = hidden_error * self.sigmoid_derivative(self.a1)\n",
    "        \n",
    "        # Step 3: Update weights and biases\n",
    "        # Update weights between hidden and output layer\n",
    "\n",
    "        '''\n",
    "        updating all the weight vectors and their biases with the computed derivates using partial derivatives formula. \n",
    "        '''\n",
    "        \n",
    "        self.W2 += self.a1.T.dot(output_delta) * self.learning_rate / m\n",
    "        self.b2 += np.sum(output_delta, axis=0, keepdims=True) * self.learning_rate / m\n",
    "        \n",
    "        # Update weights between input and hidden layer\n",
    "        self.W1 += X.T.dot(hidden_delta) * self.learning_rate / m\n",
    "        self.b1 += np.sum(hidden_delta, axis=0, keepdims=True) * self.learning_rate / m\n",
    "    \n",
    "    def train(self, X, y, epochs=1000):\n",
    "        \"\"\"\n",
    "        Train the neural network.\n",
    "        \n",
    "        Args:\n",
    "            X (numpy array): Training data\n",
    "            y (numpy array): Training labels\n",
    "            epochs (int): Number of training iterations\n",
    "            \n",
    "        Returns:\n",
    "            list: Loss values during training (for plotting)\n",
    "        \"\"\"\n",
    "        losses = []\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Forward pass: get predictions\n",
    "            _, output = self.forward(X)\n",
    "            \n",
    "            # Calculate loss (mean squared error)\n",
    "            loss = np.mean((y - output) ** 2)\n",
    "            losses.append(loss)\n",
    "            \n",
    "            # Backward pass: learn from mistakes\n",
    "            self.backward(X, y, output)\n",
    "            \n",
    "            # Print progress every 100 epochs\n",
    "            if epoch % 1000 == 0:\n",
    "                print(f\"Epoch {epoch}, Loss: {loss:.6f}\")\n",
    "        \n",
    "        return losses\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions on new unseen test data.\n",
    "        \n",
    "        Args:\n",
    "            X (numpy array): Input data\n",
    "            \n",
    "        Returns:\n",
    "            numpy array: Predictions (probabilities between 0 and 1)\n",
    "        \"\"\"\n",
    "        _, output = self.forward(X)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb91cb36-782b-4f04-aadb-84324c79f7ce",
   "metadata": {},
   "source": [
    "### For a Neural Network with:\n",
    "\n",
    "* **Input layer**\n",
    "* **k hidden layers**\n",
    "* **1 output layer**\n",
    "\n",
    "You will have:\n",
    "\n",
    "| Layer type         | Weights            | Biases | Shape of Weights               |\n",
    "| ------------------ | ------------------ | ------ | ------------------------------ |\n",
    "| Input → Hidden1    | `W1`, `b1`         |        | `(hidden1_size, input_size)`   |\n",
    "| Hidden1 → Hidden2  | `W2`, `b2`         |        | `(hidden2_size, hidden1_size)` |\n",
    "| Hidden2 → Hidden3  | `W3`, `b3`         | ...    | ...                            |\n",
    "| ...                | ...                | ...    | ...                            |\n",
    "| Hidden\\_k → Output | `W{k+1}`, `b{k+1}` |        | `(output_size, hidden_k_size)` |\n",
    "\n",
    "\n",
    "### General Rule:\n",
    "\n",
    "For `i = 1 to k+1`:\n",
    "\n",
    "* `Wi`: shape = `(layer_size[i], layer_size[i-1])`\n",
    "* `bi`: shape = `(layer_size[i], 1)`\n",
    "\n",
    "Where:\n",
    "\n",
    "* `layer_size[0] = input_size`\n",
    "* `layer_size[1] = hidden1_size`\n",
    "* `...`\n",
    "* `layer_size[k] = hidden_k_size`\n",
    "* `layer_size[k+1] = output_size`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c6768dc2-059d-40fa-8dc1-6a99febccefa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Simple Neural Network Demo ===\n",
      "\n",
      "Creating XOR dataset...\n",
      "Input data:\n",
      "[[0 0]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 1]]\n",
      "Expected outputs:\n",
      "[0 1 1 0]\n",
      "\n",
      "Creating neural network...\n",
      "Training neural network...\n",
      "\n",
      "Epoch 0, Loss: 0.278579\n",
      "Epoch 100, Loss: 0.273062\n",
      "Epoch 200, Loss: 0.268432\n",
      "Epoch 300, Loss: 0.264607\n",
      "Epoch 400, Loss: 0.261489\n",
      "Epoch 500, Loss: 0.258975\n",
      "Epoch 600, Loss: 0.256965\n",
      "Epoch 700, Loss: 0.255370\n",
      "Epoch 800, Loss: 0.254111\n",
      "Epoch 900, Loss: 0.253121\n",
      "Epoch 1000, Loss: 0.252345\n",
      "Epoch 1100, Loss: 0.251739\n",
      "Epoch 1200, Loss: 0.251264\n",
      "Epoch 1300, Loss: 0.250894\n",
      "Epoch 1400, Loss: 0.250605\n",
      "Epoch 1500, Loss: 0.250379\n",
      "Epoch 1600, Loss: 0.250203\n",
      "Epoch 1700, Loss: 0.250064\n",
      "Epoch 1800, Loss: 0.249955\n",
      "Epoch 1900, Loss: 0.249869\n",
      "Epoch 2000, Loss: 0.249801\n",
      "Epoch 2100, Loss: 0.249746\n",
      "Epoch 2200, Loss: 0.249702\n",
      "Epoch 2300, Loss: 0.249667\n",
      "Epoch 2400, Loss: 0.249638\n",
      "Epoch 2500, Loss: 0.249613\n",
      "Epoch 2600, Loss: 0.249593\n",
      "Epoch 2700, Loss: 0.249575\n",
      "Epoch 2800, Loss: 0.249560\n",
      "Epoch 2900, Loss: 0.249547\n",
      "Epoch 3000, Loss: 0.249535\n",
      "Epoch 3100, Loss: 0.249524\n",
      "Epoch 3200, Loss: 0.249514\n",
      "Epoch 3300, Loss: 0.249504\n",
      "Epoch 3400, Loss: 0.249495\n",
      "Epoch 3500, Loss: 0.249486\n",
      "Epoch 3600, Loss: 0.249478\n",
      "Epoch 3700, Loss: 0.249470\n",
      "Epoch 3800, Loss: 0.249462\n",
      "Epoch 3900, Loss: 0.249454\n",
      "Epoch 4000, Loss: 0.249446\n",
      "Epoch 4100, Loss: 0.249438\n",
      "Epoch 4200, Loss: 0.249431\n",
      "Epoch 4300, Loss: 0.249423\n",
      "Epoch 4400, Loss: 0.249415\n",
      "Epoch 4500, Loss: 0.249408\n",
      "Epoch 4600, Loss: 0.249400\n",
      "Epoch 4700, Loss: 0.249393\n",
      "Epoch 4800, Loss: 0.249385\n",
      "Epoch 4900, Loss: 0.249378\n",
      "Epoch 5000, Loss: 0.249370\n",
      "Epoch 5100, Loss: 0.249363\n",
      "Epoch 5200, Loss: 0.249355\n",
      "Epoch 5300, Loss: 0.249347\n",
      "Epoch 5400, Loss: 0.249340\n",
      "Epoch 5500, Loss: 0.249332\n",
      "Epoch 5600, Loss: 0.249325\n",
      "Epoch 5700, Loss: 0.249317\n",
      "Epoch 5800, Loss: 0.249309\n",
      "Epoch 5900, Loss: 0.249302\n",
      "Epoch 6000, Loss: 0.249294\n",
      "Epoch 6100, Loss: 0.249287\n",
      "Epoch 6200, Loss: 0.249279\n",
      "Epoch 6300, Loss: 0.249271\n",
      "Epoch 6400, Loss: 0.249263\n",
      "Epoch 6500, Loss: 0.249256\n",
      "Epoch 6600, Loss: 0.249248\n",
      "Epoch 6700, Loss: 0.249240\n",
      "Epoch 6800, Loss: 0.249232\n",
      "Epoch 6900, Loss: 0.249225\n",
      "Epoch 7000, Loss: 0.249217\n",
      "Epoch 7100, Loss: 0.249209\n",
      "Epoch 7200, Loss: 0.249201\n",
      "Epoch 7300, Loss: 0.249193\n",
      "Epoch 7400, Loss: 0.249185\n",
      "Epoch 7500, Loss: 0.249177\n",
      "Epoch 7600, Loss: 0.249169\n",
      "Epoch 7700, Loss: 0.249161\n",
      "Epoch 7800, Loss: 0.249153\n",
      "Epoch 7900, Loss: 0.249145\n",
      "Epoch 8000, Loss: 0.249137\n",
      "Epoch 8100, Loss: 0.249129\n",
      "Epoch 8200, Loss: 0.249121\n",
      "Epoch 8300, Loss: 0.249113\n",
      "Epoch 8400, Loss: 0.249105\n",
      "Epoch 8500, Loss: 0.249097\n",
      "Epoch 8600, Loss: 0.249089\n",
      "Epoch 8700, Loss: 0.249080\n",
      "Epoch 8800, Loss: 0.249072\n",
      "Epoch 8900, Loss: 0.249064\n",
      "Epoch 9000, Loss: 0.249056\n",
      "Epoch 9100, Loss: 0.249047\n",
      "Epoch 9200, Loss: 0.249039\n",
      "Epoch 9300, Loss: 0.249031\n",
      "Epoch 9400, Loss: 0.249022\n",
      "Epoch 9500, Loss: 0.249014\n",
      "Epoch 9600, Loss: 0.249005\n",
      "Epoch 9700, Loss: 0.248997\n",
      "Epoch 9800, Loss: 0.248988\n",
      "Epoch 9900, Loss: 0.248980\n",
      "\n",
      "=== Testing the trained network ===\n",
      "Input -> Expected -> Predicted -> Rounded\n",
      "[0 0] -> 0 -> 0.4670 -> 0\n",
      "[0 1] -> 1 -> 0.5163 -> 1\n",
      "[1 0] -> 1 -> 0.4874 -> 0\n",
      "[1 1] -> 0 -> 0.5302 -> 1\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final loss: 0.248971\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "# Example usage and demonstration\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=== Simple Neural Network Demo ===\\n\")\n",
    "    \n",
    "    # Create some sample data (XOR problem)\n",
    "    # XOR is a classic problem that's not linearly separable\n",
    "    print(\"Creating XOR dataset...\")\n",
    "    X = np.array([[0, 0],    # Input: [0, 0] -> Output: 0\n",
    "                  [0, 1],    # Input: [0, 1] -> Output: 1\n",
    "                  [1, 0],    # Input: [1, 0] -> Output: 1\n",
    "                  [1, 1]])   # Input: [1, 1] -> Output: 0\n",
    "    \n",
    "    y = np.array([[0],       # Expected outputs\n",
    "                  [1],\n",
    "                  [1],\n",
    "                  [0]])\n",
    "    \n",
    "    print(\"Input data:\")\n",
    "    print(X)\n",
    "    print(\"Expected outputs:\")\n",
    "    print(y.flatten())\n",
    "    print()\n",
    "    \n",
    "    # Create and train the neural network\n",
    "    print(\"Creating neural network...\")\n",
    "    # 2 inputs, 4 hidden neurons, learning rate 0.5\n",
    "    nn = SimpleNeuralNetwork(input_size=2, hidden_size=4, learning_rate=0.01)\n",
    "    \n",
    "    print(\"Training neural network...\\n\")\n",
    "    losses = nn.train(X, y, epochs=10000)\n",
    "    \n",
    "    # Test the trained network\n",
    "    print(\"\\n=== Testing the trained network ===\")\n",
    "    predictions = nn.predict(X)\n",
    "    \n",
    "    print(\"Input -> Expected -> Predicted -> Rounded\")\n",
    "    for i in range(len(X)):\n",
    "        rounded = 1 if predictions[i][0] > 0.5 else 0\n",
    "        print(f\"{X[i]} -> {y[i][0]} -> {predictions[i][0]:.4f} -> {rounded}\")\n",
    "    \n",
    "    # Plot the training loss\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(losses)\n",
    "    plt.title('Training Loss Over Time')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Mean Squared Error')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nFinal loss: {losses[-1]:.6f}\")\n",
    "    print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4741a31-05f9-4688-9894-b3a698cabeb1",
   "metadata": {},
   "source": [
    "Great — let’s walk through **backpropagation step-by-step for a neural network with**:\n",
    "\n",
    "* One **input layer**\n",
    "* One **hidden layer**\n",
    "* One **output layer**\n",
    "\n",
    "This will be a full end-to-end walkthrough — math + intuition + code-style explanation.\n",
    "\n",
    "---\n",
    "\n",
    "### 📐 Network Structure\n",
    "\n",
    "Let’s define:\n",
    "\n",
    "* Input: $X \\in \\mathbb{R}^{m \\times n}$\n",
    "  $m$: number of samples, $n$: number of input features\n",
    "* Hidden layer size: $h$\n",
    "* Output: 1 neuron (binary classification)\n",
    "\n",
    "#### Parameters:\n",
    "\n",
    "| Parameter | Shape    | Description                  |\n",
    "| --------- | -------- | ---------------------------- |\n",
    "| `W1`      | $(n, h)$ | weights from input → hidden  |\n",
    "| `b1`      | $(1, h)$ | biases for hidden layer      |\n",
    "| `W2`      | $(h, 1)$ | weights from hidden → output |\n",
    "| `b2`      | $(1, 1)$ | bias for output neuron       |\n",
    "\n",
    "#### Activations:\n",
    "\n",
    "* Hidden layer: sigmoid\n",
    "* Output layer: sigmoid\n",
    "\n",
    "---\n",
    "\n",
    "### 🔁 Forward Pass:\n",
    "\n",
    "```python\n",
    "# Input → Hidden\n",
    "z1 = X @ W1 + b1        # shape: (m, h)\n",
    "a1 = sigmoid(z1)        # shape: (m, h)\n",
    "\n",
    "# Hidden → Output\n",
    "z2 = a1 @ W2 + b2       # shape: (m, 1)\n",
    "a2 = sigmoid(z2)        # shape: (m, 1) → predictions\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 🎯 Loss (Binary Classification):\n",
    "\n",
    "$$\n",
    "L = \\frac{1}{m} \\sum_i (y_i - \\hat{y}_i)^2 \\quad \\text{or use BCE instead}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 🔁 Backward Pass:\n",
    "\n",
    "We apply the **chain rule** to propagate the error from the output back to weights:\n",
    "\n",
    "#### 1. Output Layer\n",
    "\n",
    "$$\n",
    "\\text{output\\_error} = a2 - y \\quad \\text{(shape: (m, 1))}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{output\\_delta} = \\text{output\\_error} \\cdot \\sigma'(z2)\n",
    "\\quad \\text{or if a2 already sigmoid: } \\text{output\\_delta} = (a2 - y) \\cdot a2 \\cdot (1 - a2)\n",
    "$$\n",
    "\n",
    "#### 2. Gradients for W2, b2:\n",
    "\n",
    "$$\n",
    "dW2 = a1^T @ \\text{output\\_delta} \\quad \\text{(shape: (h, 1))}\n",
    "$$\n",
    "\n",
    "$$\n",
    "db2 = \\sum(\\text{output\\_delta}, \\text{axis}=0, \\text{keepdims}=True)\n",
    "$$\n",
    "\n",
    "#### 3. Backprop to Hidden Layer\n",
    "\n",
    "$$\n",
    "\\text{hidden\\_error} = \\text{output\\_delta} @ W2^T \\quad \\text{(shape: (m, h))}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{hidden\\_delta} = \\text{hidden\\_error} \\cdot a1 \\cdot (1 - a1)\n",
    "$$\n",
    "\n",
    "#### 4. Gradients for W1, b1:\n",
    "\n",
    "$$\n",
    "dW1 = X^T @ \\text{hidden\\_delta} \\quad \\text{(shape: (n, h))}\n",
    "$$\n",
    "\n",
    "$$\n",
    "db1 = \\sum(\\text{hidden\\_delta}, \\text{axis}=0, \\text{keepdims}=True)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 🔁 Update Weights:\n",
    "\n",
    "```python\n",
    "W2 -= learning_rate * dW2\n",
    "b2 -= learning_rate * db2\n",
    "W1 -= learning_rate * dW1\n",
    "b1 -= learning_rate * db1\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 🧠 Intuition:\n",
    "\n",
    "* Error is measured at the output\n",
    "* We compute how much each weight *contributed* to that error using gradients\n",
    "* Then we nudge each weight in the direction that reduces that error\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92b77f9-206e-4c5d-813c-7323ac489a59",
   "metadata": {},
   "source": [
    "You're on the right track—and your curiosity is exactly how a deep understanding of backpropagation begins. Let's walk through this from a **math-first** perspective.\n",
    "\n",
    "---\n",
    "\n",
    "### 🌱 Problem Setup\n",
    "\n",
    "We have a **2-layer neural network**:\n",
    "\n",
    "* **Input → Hidden → Output**\n",
    "* Let:\n",
    "\n",
    "  * $X \\in \\mathbb{R}^{m \\times n}$: Input matrix, $m$ samples, $n$ features\n",
    "  * $W_1 \\in \\mathbb{R}^{n \\times h}$: Weights from input to hidden\n",
    "  * $b_1 \\in \\mathbb{R}^{1 \\times h}$: Bias for hidden layer\n",
    "  * $a_1 = \\sigma(Z_1)$, where $Z_1 = XW_1 + b_1$\n",
    "  * $W_2 \\in \\mathbb{R}^{h \\times 1}$: Weights from hidden to output\n",
    "  * $b_2 \\in \\mathbb{R}^{1 \\times 1}$: Bias for output\n",
    "  * $\\hat{y} = \\sigma(Z_2)$, where $Z_2 = a_1 W_2 + b_2$\n",
    "\n",
    "---\n",
    "\n",
    "### 🎯 Loss Function\n",
    "\n",
    "Assume we're using **binary cross-entropy loss**:\n",
    "\n",
    "$$\n",
    "L(y, \\hat{y}) = -[y \\log(\\hat{y}) + (1 - y) \\log(1 - \\hat{y})]\n",
    "$$\n",
    "\n",
    "The objective is to **compute the partial derivatives** of this loss w\\.r.t. all learnable parameters $W_1, b_1, W_2, b_2$.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔁 Backpropagation: Chain Rule in Action\n",
    "\n",
    "Let’s denote:\n",
    "\n",
    "* $\\hat{y} = \\sigma(Z_2)$\n",
    "* $Z_2 = a_1 W_2 + b_2$\n",
    "* $a_1 = \\sigma(Z_1)$\n",
    "* $Z_1 = XW_1 + b_1$\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 Step 1: Output Delta\n",
    "\n",
    "We compute the gradient of the loss with respect to the output layer pre-activation $Z_2$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial Z_2} = \\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial Z_2}\n",
    "$$\n",
    "\n",
    "* For binary cross-entropy + sigmoid combo:\n",
    "\n",
    "  $$\n",
    "  \\frac{\\partial L}{\\partial Z_2} = \\hat{y} - y\n",
    "  $$\n",
    "\n",
    "So in code:\n",
    "\n",
    "```python\n",
    "output_error = y - output       # This is -(∂L/∂Z2)\n",
    "output_delta = output_error * sigmoid_derivative(output)\n",
    "```\n",
    "\n",
    "This is correct because $\\text{sigmoid}'(z) = \\hat{y}(1 - \\hat{y})$. So multiplying the error with sigmoid derivative computes the gradient w\\.r.t. $Z_2$.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 Step 2: Hidden Delta\n",
    "\n",
    "Now we propagate the error to the hidden layer using:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial Z_1} = \\left( \\frac{\\partial L}{\\partial Z_2} \\cdot W_2^T \\right) \\cdot \\sigma'(Z_1)\n",
    "$$\n",
    "\n",
    "In code:\n",
    "\n",
    "```python\n",
    "hidden_error = output_delta.dot(W2.T)\n",
    "hidden_delta = hidden_error * sigmoid_derivative(a1)\n",
    "```\n",
    "\n",
    "\n",
    "Here’s what’s happening:\n",
    "\n",
    "* `output_delta.dot(W2.T)` gives us $\\partial L/\\partial a_1$\n",
    "* Multiplying by $\\sigma'(Z_1)$ gives $\\partial L/\\partial Z_1$\n",
    "\n",
    "---\n",
    "\n",
    "### 🔄 Step 3: Update Parameters\n",
    "\n",
    "We now update weights and biases using gradient descent:\n",
    "\n",
    "* $W_2 := W_2 - \\eta \\cdot \\frac{\\partial L}{\\partial W_2}$\n",
    "* $b_2 := b_2 - \\eta \\cdot \\frac{\\partial L}{\\partial b_2}$\n",
    "* $W_1 := W_1 - \\eta \\cdot \\frac{\\partial L}{\\partial W_1}$\n",
    "* $b_1 := b_1 - \\eta \\cdot \\frac{\\partial L}{\\partial b_1}$\n",
    "\n",
    "With:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W_2} = a_1^T \\cdot output\\_delta\n",
    "\\quad \\text{and} \\quad\n",
    "\\frac{\\partial L}{\\partial W_1} = X^T \\cdot hidden\\_delta\n",
    "$$\n",
    "\n",
    "And same for biases:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial b_2} = \\sum output\\_delta, \\quad\n",
    "\\frac{\\partial L}{\\partial b_1} = \\sum hidden\\_delta\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Summary of `output_delta` and `hidden_delta`\n",
    "\n",
    "* `output_delta` represents:\n",
    "\n",
    "  $$\n",
    "  \\frac{\\partial L}{\\partial Z_2}\n",
    "  $$\n",
    "\n",
    "  — how much should the output *neuron* adjust its activation.\n",
    "\n",
    "* `hidden_delta` represents:\n",
    "\n",
    "  $$\n",
    "  \\frac{\\partial L}{\\partial Z_1}\n",
    "  $$\n",
    "\n",
    "  — how much should the hidden *neurons* adjust to reduce the final loss.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91a6a59-d241-4a02-be8f-a41e69ca17ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264e9720-2924-4cef-9fd7-709def976477",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
